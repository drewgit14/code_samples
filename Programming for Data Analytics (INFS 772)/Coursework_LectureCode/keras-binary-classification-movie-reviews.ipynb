{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<p style=\"font-family: Arial; font-size:3em;color:purple; font-style:bold\"><br>\n",
    "Binary Classification of Movie Reviews with Keras<br><br>\n",
    "<br><br>INFS 772 Spring 2019 ~ Week 15-16<br></p>\n",
    "\n",
    "<p style=\"font-family: Arial; font-size:1.5em;color:#2462C0; font-style:bold\"><br>\n",
    "IMDB Movie reviews sentiment classification</p>\n",
    "\n",
    "- Load the IMDB dataset\n",
    "- Define Training Data: input tensors and target tensors\n",
    "- Define a network of layers (a \"model\") that map inputs to targets\n",
    "- Configue the learning process by picking a loss function, an optimizer, and some metrics to monitor\n",
    "- Iterate on the training data\n",
    "- Verify accuracy\n",
    "- Optimize\n",
    "- Predict\n",
    "\n",
    "<p style=\"font-family: Arial; font-size:1.5em;color:#2462C0; font-style:bold\"><br>\n",
    "One-Dimensional Convolutional Neural Network Model for the IMDB Dataset</p>\n",
    "<p style=\"font-family: Arial; font-size:1.5em;color:#2462C0; font-style:bold\"><br>\n",
    "A LSTM Model on the IMDB Sentiment Classification</p>\n",
    "<p style=\"font-family: Arial; font-size:1.5em;color:#2462C0; font-style:bold\"><br>\n",
    "LSTM and Convolutional Neural Network For Sequence Classification</p>\n",
    "<p style=\"font-family: Arial; font-size:1.5em;color:#2462C0; font-style:bold\"><br>\n",
    "Summary</p>\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "<center>\n",
    "    <br>\n",
    "    <p style=\"font-family: Arial; font-size:1.5em;color:#2462C0; font-style:bold\"></p><br><br>\n",
    "    <br><br>\n",
    "    <img src='https://opensource.com/sites/default/files/u128651/1tensorflow.png' width=40%>\n",
    "</center>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## IMDB Movie reviews sentiment classification\n",
    "\n",
    "#### After this last part of the workshop, you will know:\n",
    "\n",
    "- About the IMDB dataset for sentiment analysis and how to load it in Keras.\n",
    "- How to use word embedding in Keras for natural language problems.\n",
    "- How to develop and evaluate a multi-layer neural network model for the IMDB problem.\n",
    "- How to develop a one-dimensional convolutional neural network model for the IMDB problem.\n",
    "\n",
    "In this tutorial we will use the IMDB Movie Review Dataset (from Stanford University) which consists of 50,000 movie reviews (50% negative and 50% positive). The set is divided to training and test datasets (each with 25000 movie reviews with equal number of positive and negative reviews).\n",
    "\n",
    "Our objective is to create a neural network (as a Keras model) that can predict if a given movie review is positive or negative. This type of computation is also called Sentiment Analysis, as we try to train our neural network to predict the sentimental value of a movie review (negative or positive?).\n",
    "\n",
    "Reviews have been preprocessed, and each review is encoded as a sequence of word indexes (integers). For convenience, words are indexed by overall frequency in the dataset, so that for instance the integer \"3\" encodes the 3rd most frequent word in the data. This allows for quick filtering operations such as: \"only consider the top 10,000 most common words\".\n",
    "\n",
    "The data was collected by Stanford researchers and was used in a 2011 paper where a split of 50/50 of the data was used for training and test. An accuracy of 88.89% was achieved.\n",
    "\n",
    "The data was also used as the basis for a Kaggle competition titled “Bag of Words Meets Bags of Popcorn” in late 2014 to early 2015. Accuracy was achieved above 97% with winners achieving 99%.\n",
    "\n",
    "## Load the IMDB dataset\n",
    "\n",
    "### conda install tensorflow\n",
    "### conda install -c conda-forge keras"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "from keras.datasets import imdb"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The variables train_data and test_data are lists of movie reviews. Each review is a\n",
    "list of word indices (encoding a sequence of words). train_labels and test_labels\n",
    "are lists of 0's and 1's. 0 stands for \"negative\" and 1 stands for \"positive\"."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Volume in drive C has no label.\n",
      " Volume Serial Number is 644B-57E9\n",
      "\n",
      " Directory of C:\\Users\\dzeng\\.keras\n",
      "\n",
      "09/20/2017  02:29 PM    <DIR>          .\n",
      "09/20/2017  02:29 PM    <DIR>          ..\n",
      "02/05/2018  12:16 PM    <DIR>          datasets\n",
      "09/20/2017  02:26 PM               125 keras.json\n",
      "               1 File(s)            125 bytes\n",
      "               3 Dir(s)  419,874,766,848 bytes free\n"
     ]
    }
   ],
   "source": [
    "!dir C:\\Users\\dzeng\\.keras"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The argument num_words=10000 means to only keep the 10,000 most frequently occurring words in the training data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "(train_data, train_labels), (test_data, test_labels) = imdb.load_data(num_words=10000)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**imdb.load_data returns:**\n",
    "\n",
    "train_data, test_data: list of sequences, which are lists of indexes (integers). If the num_words argument was specific, the maximum possible index value is num_words-1.\n",
    "train_labels, test_labels: list of integer labels (1 or 0).\n",
    "\n",
    "Here's the first record in train_data...."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1, 4, 9941, 132, 1184, 5622, 1935, 9, 34, 57, 817, 6, 52, 20, 21, 13, 191, 66, 135, 45, 31, 13, 2598, 149, 13, 100, 140, 23, 5, 23, 44, 4, 1566, 1409, 7, 4, 20, 40, 4, 394, 116, 5, 4, 4673, 139, 121, 1935, 9, 267, 18, 4, 250, 47, 6, 2, 1477, 34, 160, 2, 1477, 34, 6, 925, 720, 19, 6, 2, 525, 21, 45, 43, 24, 290, 2, 23, 54, 12, 266, 8, 6, 20, 40, 14, 305, 1977, 677, 1007, 138, 25, 144, 106, 4, 9941, 132, 60, 151, 45, 78, 10, 10, 457, 45, 254, 8, 6110, 15, 12, 47, 49, 2070, 948, 1008, 8, 12, 4, 64, 439, 9, 11, 94, 954, 3386, 163, 2603, 48, 164, 334, 14, 9, 6, 20, 15, 203, 6021, 25, 8, 67, 4, 204, 4827, 22, 42, 60, 332, 4, 346, 65, 23, 63, 12, 9, 448, 10, 10, 470, 18, 6, 954, 189, 708, 12, 9, 66, 2, 5754, 45, 184, 578, 15, 12, 16, 814, 23, 1622, 305, 7, 772, 1419, 268, 42, 3761, 732, 38, 75, 79, 8, 67, 49, 55, 87, 1385, 50, 26, 82, 111, 1780, 3946, 665, 12, 9, 6, 55, 52, 267, 20, 10, 10, 342, 5622, 1935, 9, 24, 38, 76, 35, 284, 17, 29, 9, 6, 1147, 7, 876, 726, 335, 6, 337, 7, 27, 42, 24, 12, 186, 17, 48, 45, 1167, 18, 1935, 8, 297, 6, 1280, 232, 50, 9, 210, 49, 243, 7, 2, 42, 2, 29, 961, 8, 4, 105, 29, 299, 5, 1276, 13, 244, 210, 4733, 34, 149, 90, 11, 101, 20, 29, 127, 726, 5622, 1935, 9, 87, 42, 394, 29, 210, 961, 27, 955, 1708, 83, 297, 5, 29, 9, 115, 357, 8, 106, 29, 9, 394, 11, 4, 9941, 132, 21, 11, 4, 91, 389, 243, 7, 96, 10, 10, 241, 6, 1475, 100, 242, 901, 6, 609, 7, 6, 2300, 23, 14, 20, 17, 12, 186, 8, 30, 4, 5655, 1210, 4944, 20, 126, 93, 67, 25, 100, 901, 14, 9, 51, 571, 54, 372, 26, 1668, 8, 521, 6, 926, 82, 4, 4351, 200, 14, 2, 926, 5, 6, 2, 8184, 26, 184, 221, 5, 290, 8102, 10, 10, 300, 48, 335, 886, 14, 225, 242, 6, 52, 580, 25, 203, 28, 110, 6, 6221, 374, 15, 47, 413, 55, 1063, 6, 1594, 7, 3568, 39, 4, 20, 587, 1935, 620, 187, 11, 6, 2123, 1735, 5, 7, 265, 4, 3149, 2, 4, 2, 61, 523, 347, 134, 139, 26, 642, 46, 7, 2008, 5, 36, 26, 131, 1018, 163, 137, 149, 98, 11, 4, 598, 6274, 10, 10, 13, 1247, 4, 343, 288, 33, 2645, 18, 44, 457, 2512, 54, 25, 157, 15, 46, 45, 44, 6, 2863, 3113, 282, 45, 6, 184, 52, 855, 10, 10, 854, 4, 2, 310, 7, 4, 20, 9, 4, 118, 8, 106, 5, 45, 128, 8, 106, 4, 2253, 310, 43, 18, 94, 117, 1283, 23, 2, 63, 944, 6, 2055, 39, 592, 4748]\n"
     ]
    }
   ],
   "source": [
    "print(train_data[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "What we see is a list of integers. Each integer represents one word in a movie review. That is, our movie review has been converted to a list of integers. Each word in our movie review vocabulary has been indexed by a unique integer. Here is the text of an example review from our dataset:\n",
    "<img src='http://www.samyzaf.com/ML/imdb/review1.png' width=70%>\n",
    "\n",
    "A simple parsing and indexing can map each word in our movie review database to a unique integer like in this example:\n",
    "<img src='http://www.samyzaf.com/ML/imdb/review2.png' width=70%>\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((25000,), (25000,))"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_data.shape, test_data.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And the first label in train_labels...."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "train_data[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Define Training Data: input tensors and target tensors\n",
    "We can't feed lists of integers into a neural network. The lists must be converted to tensors. Here's one approach to do that."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "def vectorize_sequences(sequences, dimension=10000):\n",
    "    # first initialize a tensor of shape (len(sequences), dimension) with all zeros\n",
    "    results = np.zeros((len(sequences), dimension))\n",
    "    \n",
    "    # now loop through and set integers (i.e. words) to 1.\n",
    "    for i, sequence in enumerate(sequences):\n",
    "        results[i, sequence] = 1. \n",
    "    \n",
    "    return results\n",
    "\n",
    "# vectorized training data\n",
    "x_train = vectorize_sequences(train_data)\n",
    "\n",
    "# vectorized test data\n",
    "x_test = vectorize_sequences(test_data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here's the first integer sequence encoded into a binary tensor."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 0.,  1.,  1., ...,  0.,  0.,  0.])"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x_train[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "10000"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(x_train[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We also need to vectorize the labels."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "y_train = np.asarray(train_labels).astype('float32')\n",
    "y_test = np.asarray(test_labels).astype('float32')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src='http://deeplearning.lipingyang.org/wp-content/uploads/2017/02/img_589a484724625.png' width=25%>\n",
    "## Define a network of layers (a \"model\") that map inputs to targets\n",
    "Now we can create a network model. In this example, we'll try two intermediate layers with 16 hidden units each, and a third layer which will output the scalar prediction of the sentiment (positive or negative) of the current review. The intermediate layers will use relu as the activation function. The final layer will use sigmoid activation to generate a probability (a score between 0 and 1) which indicates how likely the sample is be a positive review (the closer the score is to 1, the higher the probability that the review is positive)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "dense_4 (Dense)              (None, 16)                160016    \n",
      "_________________________________________________________________\n",
      "dense_5 (Dense)              (None, 16)                272       \n",
      "_________________________________________________________________\n",
      "dense_6 (Dense)              (None, 1)                 17        \n",
      "=================================================================\n",
      "Total params: 160,305\n",
      "Trainable params: 160,305\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "from keras.models import Sequential\n",
    "from keras.layers import Dense\n",
    "\n",
    "model = Sequential()\n",
    "model.add(Dense(16, activation='relu', input_dim=10000))\n",
    "model.add(Dense(16, activation='relu'))\n",
    "model.add(Dense(1, activation='sigmoid'))\n",
    "print(model.summary())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src='https://cdn-images-1.medium.com/max/1200/1*FcEfcrucAFymCr0gMFQ0QA@2x.png' width=40%>\n",
    "<center style=\"font-family: Arial; font-size:1.3em;color:blue; font-style:bold\">The structure of one neuron connecting to the inputs<br>n = 10000 in our model</center><br><br><br><br>\n",
    "<img src='https://cdn-images-1.medium.com/max/1200/1*Gh5PS4R_A5drl5ebd_gNrg@2x.png' width=50%>\n",
    "<center style=\"font-family: Arial; font-size:1.3em;color:blue; font-style:bold\">The network architecture<br>We have 16 and 16 nodes for the two hidden layers in our model</center><br><br><br><br>\n",
    "<img src='https://cdn-images-1.medium.com/max/1200/1*4u9oFYoxIwqHzOfGY-4W-w.png' width=60%><br>\n",
    "<img src='https://cdn-images-1.medium.com/max/1200/1*scEV0vy3N-bQBVlv2pjdoQ.png' width=60%>\n",
    "<center style=\"font-family: Arial; font-size:1.3em;color:blue; font-style:bold\">The data transformation<br>We have 10000X16 + 16X16 +16X1 = 160,272 weights, and total 160,305 parameters in our model</center><br><br><br><br>\n",
    "\n",
    "\n",
    "## Configue the learning process by picking a loss function, an optimizer, and some metrics to monitor\n",
    "\n",
    "\n",
    "We also need to select a loss function and an optimizer for the network. We'll choose the rmsprop optimizer and the binary_crossentropy loss function. We also want to measure accuracy."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "model.compile(optimizer='rmsprop', loss='binary_crossentropy', metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We'll also configure the optimizer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from keras.optimizers import RMSprop\n",
    "\n",
    "model.compile(optimizer=RMSprop(lr=0.001), loss='binary_crossentropy', metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We also want binary accuracy."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from keras.objectives import binary_crossentropy\n",
    "from keras.metrics import binary_accuracy\n",
    "\n",
    "model.compile(optimizer=RMSprop(lr=0.001), loss=binary_crossentropy, metrics=[binary_accuracy])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we'll create a set of validation data by with 10,000 samples from the original training data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_val = x_train[:10000]\n",
    "partial_x_train = x_train[10000:]\n",
    "\n",
    "y_val = y_train[:10000]\n",
    "partial_y_train = y_train[10000:]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Iterate on the training data\n",
    "Now we'll train our model for 20 epochs (i.e., 20 iterations over all samples in the x_train and y_train tensors), in batches of 512 samples. We'll also monitor loss and accuracy on the 10,000 samples by passing the validation data as the validation_data argument.\n",
    "<br>**During each iteration, there are 160,305 parameter updates. To complete 20 epochs on the 15,000 training samples, there are about 94 million parameter updates!**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "history = model.fit(partial_x_train, partial_y_train, epochs=20, batch_size=512, validation_data=(x_val, y_val))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Verify accuracy\n",
    "Now we can look at the training metrics."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "dict_keys(['val_loss', 'val_binary_accuracy', 'loss', 'binary_accuracy'])"
      ]
     },
     "execution_count": 79,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "history_dict = history.history\n",
    "history_dict.keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAYUAAAEKCAYAAAD9xUlFAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x1ee196d1048>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "\n",
    "loss_values = history_dict['loss']\n",
    "val_loss_values = history_dict['val_loss']\n",
    "epochs = range(1, len(loss_values) + 1)\n",
    "\n",
    "plt.plot(epochs, loss_values, 'bo')\n",
    "plt.plot(epochs, val_loss_values, 'b+')\n",
    "plt.xlabel('Epochs')\n",
    "plt.ylabel('Loss')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAYsAAAEKCAYAAADjDHn2AAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nI0Gh+YT7AAAAAElFTkSuQmCC\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x1ef45966e80>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.clf() \n",
    "\n",
    "acc_values = history_dict['binary_accuracy']\n",
    "val_acc_values = history_dict['val_binary_accuracy']\n",
    "plt.plot(epochs, acc_values, 'bo')\n",
    "plt.plot(epochs, val_acc_values, 'b+')\n",
    "plt.xlabel('Epochs')\n",
    "plt.ylabel('Accuracy')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Optimize\n",
    "\n",
    "The graphs above show that the network is overfitting the training data after the fourth epoch. We'll try re-running the network with only four epochs this time."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/4\n",
      "25000/25000 [==============================] - 4s 174us/step - loss: 0.4356 - acc: 0.8269\n",
      "Epoch 2/4\n",
      "25000/25000 [==============================] - 3s 140us/step - loss: 0.2483 - acc: 0.9112\n",
      "Epoch 3/4\n",
      "25000/25000 [==============================] - 3s 138us/step - loss: 0.1953 - acc: 0.9304\n",
      "Epoch 4/4\n",
      "25000/25000 [==============================] - 4s 145us/step - loss: 0.1625 - acc: 0.9423\n",
      "25000/25000 [==============================] - 6s 226us/step\n"
     ]
    }
   ],
   "source": [
    "model = Sequential()\n",
    "\n",
    "model.add(Dense(16, activation='relu', input_dim=10000))\n",
    "model.add(Dense(16, activation='relu'))\n",
    "model.add(Dense(1, activation='sigmoid'))\n",
    "\n",
    "model.compile(optimizer='rmsprop', loss='binary_crossentropy', metrics=['accuracy'])\n",
    "model.fit(x_train, y_train, epochs=4, batch_size=512)\n",
    "\n",
    "results = model.evaluate(x_test, y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "model.evaluate returns the loss value & metrics values for the model in test mode."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[0.32758539591312408, 0.87212000000000001]"
      ]
     },
     "execution_count": 83,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "results"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Predict\n",
    "We can see each prediction in the test data. The closer the score is to 1, the higher the probability that the review is positive."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 0.97499543],\n",
       "       [ 0.02684157],\n",
       "       [ 0.87513351],\n",
       "       ..., \n",
       "       [ 0.00159326],\n",
       "       [ 0.0013807 ],\n",
       "       [ 0.98734295]], dtype=float32)"
      ]
     },
     "execution_count": 84,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.predict(x_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## One-Dimensional Convolutional Neural Network Model for the IMDB Dataset\n",
    "<p style=\"font-family: Arial; font-size:1.5em;color:#2462C0; font-style:bold\"><br>\n",
    "Advanced Topic</p>\n",
    "\n",
    "<img src='http://cs231n.github.io/assets/nn1/neural_net2.jpeg' width=40%><img src='http://cs231n.github.io/assets/cnn/cnn.jpeg'>\n",
    "<center>Top: A regular 3-layer Neural Network. Bottom: A ConvNet arranges its neurons in three dimensions (width, height, depth), as visualized in one of the layers. Every layer of a ConvNet transforms the 3D input volume to a 3D output volume of neuron activations. In this example, the red input layer holds the image, so its width and height would be the dimensions of the image, and the depth would be 3 (Red, Green, Blue channels).</center><br><br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Convolutional neural networks were designed to honor the spatial structure in image data whilst being robust to the position and orientation of learned objects in the scene.\n",
    "\n",
    "This same principle can be used on sequences, such as the one-dimensional sequence of words in a movie review. The same properties that make the CNN model attractive for learning to recognize objects in images can help to learn structure in paragraphs of words, namely the techniques invariance to the specific position of features.\n",
    "\n",
    "Keras supports one-dimensional convolutions and pooling by the Conv1D and MaxPooling1D classes respectively.\n",
    "\n",
    "Again, let’s import the classes and functions needed for this example."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# CNN for the IMDB problem\n",
    "#import numpy\n",
    "from keras.datasets import imdb\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, Dropout, Activation\n",
    "from keras.layers import Flatten\n",
    "from keras.layers.convolutional import Conv1D\n",
    "from keras.layers.convolutional import MaxPooling1D\n",
    "from keras.layers.embeddings import Embedding\n",
    "from keras.preprocessing import sequence\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**We are only interested in the first 10,000 most used words in the dataset. Therefore our vocabulary size will be 10,000. We can choose to use a 32-dimension vector to represent each word. Finally, we may choose to cap the maximum review length at 500 words, truncating reviews longer than that and padding reviews shorter than that with 0s.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# load the dataset but only keep the top n words, zero the rest\n",
    "top_words = 10000\n",
    "(X_train, y_train), (X_test, y_test) = imdb.load_data(num_words=top_words)\n",
    "\n",
    "# pad dataset to a maximum review length in words\n",
    "# We would then use the Keras utility to truncate or pad the dataset to a length of 500 for each observation\n",
    "# using the sequence.pad_sequences() function.\n",
    "\n",
    "max_words = 500\n",
    "X_train = sequence.pad_sequences(X_train, maxlen=max_words)\n",
    "X_test = sequence.pad_sequences(X_test, maxlen=max_words)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can now define our convolutional neural network model. This time, after the Embedding input layer, we insert a Conv1D layer. This convolutional layer has 32 feature maps and reads embedded word representations 3 vector elements of the word embedding at a time.\n",
    "\n",
    "The first layer is an embedding layer in which word indices that come from the input layer are converted to word vectors (word2vec). This is an important conversion which enables a more efficient and faster processing of textual data. Each word integer is mapped into a one dimensional vector of floats which captures its syntactical properties within the movie reviews text corpus.\n",
    "\n",
    "After the embedding layer, comes convolutional layer. Since our word vectors are one dimensional, we only need 1-dim convolutions. Keras provides us with a built-in method for doing it elegantly. Note that we need to specify a convolution kernel length and number of filters to use.\n",
    "\n",
    "The 1d convolutional layer transforms an input 2D matrix (500X32) to an output 2D matrix (500X32) with some differentiable function.\n",
    "\n",
    "The convolutional layer is followed by a 1D max pooling layer with a length and stride of 2 that halves the size of the feature maps from the convolutional layer. The rest of the network is similar to the regular neural network above."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "embedding_4 (Embedding)      (None, 500, 32)           320000    \n",
      "_________________________________________________________________\n",
      "conv1d_4 (Conv1D)            (None, 500, 32)           3104      \n",
      "_________________________________________________________________\n",
      "max_pooling1d_4 (MaxPooling1 (None, 250, 32)           0         \n",
      "_________________________________________________________________\n",
      "flatten_4 (Flatten)          (None, 8000)              0         \n",
      "_________________________________________________________________\n",
      "dense_25 (Dense)             (None, 250)               2000250   \n",
      "_________________________________________________________________\n",
      "dropout_4 (Dropout)          (None, 250)               0         \n",
      "_________________________________________________________________\n",
      "activation_4 (Activation)    (None, 250)               0         \n",
      "_________________________________________________________________\n",
      "dense_26 (Dense)             (None, 1)                 251       \n",
      "=================================================================\n",
      "Total params: 2,323,605\n",
      "Trainable params: 2,323,605\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "# create the model\n",
    "model = Sequential()\n",
    "\n",
    "#Embedding Layer\n",
    "#The output of this embedding layer would be a matrix with the size 500X32\n",
    "#for a given review training or test pattern in integer format.\n",
    "model.add(Embedding(top_words, 32, input_length=max_words))\n",
    "\n",
    "#Convolution1D Layer\n",
    "#This convolutional layer has 32 feature maps and reads embedded word representations:\n",
    "#3 vector elements of the word embedding at a time.\n",
    "model.add(Conv1D(filters=32, kernel_size=3, padding='same', activation='relu'))\n",
    "\n",
    "#Maxpooling Layer\n",
    "model.add(MaxPooling1D(pool_size=2))\n",
    "model.add(Flatten())\n",
    "\n",
    "#Dense Layer\n",
    "model.add(Dense(250))\n",
    "model.add(Dropout(0.2))\n",
    "model.add(Activation('relu'))\n",
    "\n",
    "#Output Layer\n",
    "model.add(Dense(1, activation='sigmoid'))\n",
    "\n",
    "model.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
    "print(model.summary())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**We defined a Keras sequential model (feed forward neural network) with four hidden layers as in the following diagram:**\n",
    "<img src='http://www.samyzaf.com/ML/imdb/cnn4.png' width=60%>\n",
    "<center style=\"font-family: Arial; font-size:1.3em;color:blue; font-style:bold\">The data transformation<br>10000X1 -> 500X32 -> 500X32 -> 250X32 -> 8000X1 -> 250X1 -> 1</center><br><br><br><br>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 25000 samples, validate on 25000 samples\n",
      "Epoch 1/2\n",
      " - 58s - loss: 0.4320 - acc: 0.7690 - val_loss: 0.2658 - val_acc: 0.8891\n",
      "Epoch 2/2\n",
      " - 58s - loss: 0.1887 - acc: 0.9286 - val_loss: 0.3248 - val_acc: 0.8658\n",
      "Accuracy: 86.58%\n"
     ]
    }
   ],
   "source": [
    "# Fit the model\n",
    "model.fit(X_train, y_train, validation_data=(X_test, y_test), epochs=2, batch_size=128, verbose=2)\n",
    "# Final evaluation of the model\n",
    "scores = model.evaluate(X_test, y_test, verbose=0)\n",
    "print(\"Accuracy: %.2f%%\" % (scores[1]*100))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Running the example, we first presented with a summary of the network structure. We can see our convolutional layer preserves the dimensionality of our Embedding input layer of 32-dimensional input with a maximum of 500 words. The pooling layer compresses this representation by halving it."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## A LSTM Model on the IMDB Sentiment Classification\n",
    "<p style=\"font-family: Arial; font-size:1.5em;color:#2462C0; font-style:bold\"><br>\n",
    "Advanced Topic</p>\n",
    "\n",
    "### Recurrent neural networks contain cycles that feed the network activations from a previous time step as inputs to the network to influence predictions at the current time step. These activations are stored in the internal states of the network which can in principle hold long-term temporal contextual information. This mechanism allows RNNs to exploit a dynamically changing contextual window over the input sequence history.\n",
    "\n",
    "\n",
    "We can now define, compile and fit our LSTM model.\n",
    "\n",
    "The first layer is the Embedded layer that uses 128 length vectors to represent each word. The next layer is the LSTM layer with 128 memory units (smart neurons). Finally, because this is a classification problem we use a Dense output layer with a single neuron and a sigmoid activation function to make 0 or 1 predictions for the two classes (good and bad) in the problem.\n",
    "\n",
    "Because it is a binary classification problem, log loss is used as the loss function (binary_crossentropy in Keras). The efficient ADAM optimization algorithm is used. The model is fit for only 2 epochs because it quickly overfits the problem. A batch size of 32 reviews is used to space out weight updates."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading data...\n",
      "25000 train sequences\n",
      "25000 test sequences\n",
      "Pad sequences (samples x time)\n",
      "x_train shape: (25000, 80)\n",
      "x_test shape: (25000, 80)\n",
      "Build model...\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "embedding_6 (Embedding)      (None, None, 128)         2560000   \n",
      "_________________________________________________________________\n",
      "lstm_6 (LSTM)                (None, 128)               131584    \n",
      "_________________________________________________________________\n",
      "dense_6 (Dense)              (None, 1)                 129       \n",
      "=================================================================\n",
      "Total params: 2,691,713\n",
      "Trainable params: 2,691,713\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "'''Trains an LSTM model on the IMDB sentiment classification task.\n",
    "The dataset is actually too small for LSTM to be of any advantage\n",
    "compared to simpler, much faster methods such as TF-IDF + LogReg.\n",
    "e.g. sklearn.feature_extraction.text.TfidfVectorizer\n",
    "# Notes\n",
    "- RNNs are tricky. Choice of batch size is important,\n",
    "choice of loss and optimizer is critical, etc.\n",
    "Some configurations won't converge.\n",
    "- LSTM loss decrease patterns during training can be quite different\n",
    "from what you see with CNNs/MLPs/etc.\n",
    "'''\n",
    "from __future__ import print_function\n",
    "\n",
    "from keras.preprocessing import sequence\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, Embedding\n",
    "from keras.layers import LSTM\n",
    "from keras.datasets import imdb\n",
    "\n",
    "max_features = 20000\n",
    "maxlen = 80  # cut texts after this number of words (among top max_features most common words)\n",
    "batch_size = 128\n",
    "\n",
    "print('Loading data...')\n",
    "(x_train, y_train), (x_test, y_test) = imdb.load_data(num_words=max_features)\n",
    "print(len(x_train), 'train sequences')\n",
    "print(len(x_test), 'test sequences')\n",
    "\n",
    "print('Pad sequences (samples x time)')\n",
    "x_train = sequence.pad_sequences(x_train, maxlen=maxlen)\n",
    "x_test = sequence.pad_sequences(x_test, maxlen=maxlen)\n",
    "print('x_train shape:', x_train.shape)\n",
    "print('x_test shape:', x_test.shape)\n",
    "\n",
    "print('Build model...')\n",
    "model = Sequential()\n",
    "model.add(Embedding(max_features, 128))\n",
    "model.add(LSTM(128, dropout=0.2, recurrent_dropout=0.2))\n",
    "model.add(Dense(1, activation='sigmoid'))\n",
    "\n",
    "# try using different optimizers and different optimizer configs\n",
    "model.compile(loss='binary_crossentropy',\n",
    "              optimizer='adam',\n",
    "              metrics=['accuracy'])\n",
    "\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 25000 samples, validate on 25000 samples\n",
      "Epoch 1/2\n",
      "25000/25000 [==============================] - 150s 6ms/step - loss: 0.4772 - acc: 0.7651 - val_loss: 0.3810 - val_acc: 0.8323\n",
      "Epoch 2/2\n",
      "25000/25000 [==============================] - 144s 6ms/step - loss: 0.3041 - acc: 0.8768 - val_loss: 0.3769 - val_acc: 0.8317\n",
      "25000/25000 [==============================] - 31s 1ms/step\n",
      "Test score: 0.3769398599052429\n",
      "Test accuracy: 0.8317199999809265\n"
     ]
    }
   ],
   "source": [
    "model.fit(x_train, y_train,\n",
    "          batch_size=batch_size,\n",
    "          epochs=2,\n",
    "          validation_data=(x_test, y_test))\n",
    "score, acc = model.evaluate(x_test, y_test,\n",
    "                            batch_size=batch_size)\n",
    "print('Test score:', score)\n",
    "print('Test accuracy:', acc)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## LSTM and Convolutional Neural Network For Sequence Classification\n",
    "<p style=\"font-family: Arial; font-size:1.5em;color:#2462C0; font-style:bold\"><br>\n",
    "Advanced Topic</p>\n",
    "\n",
    "Convolutional neural networks excel at learning the spatial structure in input data.\n",
    "\n",
    "The IMDB review data does have a one-dimensional spatial structure in the sequence of words in reviews and the CNN may be able to pick out invariant features for good and bad sentiment. This learned spatial features may then be learned as sequences by an LSTM layer.\n",
    "\n",
    "We can easily add a one-dimensional CNN and max pooling layers after the Embedding layer which then feed the consolidated features to the LSTM. We can use a smallish set of 32 features with a small filter length of 3. The pooling layer can use the standard length of 2 to halve the feature map size."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "embedding_3 (Embedding)      (None, 500, 32)           320000    \n",
      "_________________________________________________________________\n",
      "conv1d_2 (Conv1D)            (None, 500, 32)           3104      \n",
      "_________________________________________________________________\n",
      "max_pooling1d_2 (MaxPooling1 (None, 250, 32)           0         \n",
      "_________________________________________________________________\n",
      "lstm_3 (LSTM)                (None, 100)               53200     \n",
      "_________________________________________________________________\n",
      "dense_3 (Dense)              (None, 1)                 101       \n",
      "=================================================================\n",
      "Total params: 376,405\n",
      "Trainable params: 376,405\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "# LSTM and CNN for sequence classification in the IMDB dataset\n",
    "import numpy\n",
    "from keras.datasets import imdb\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense\n",
    "from keras.layers import LSTM\n",
    "from keras.layers.convolutional import Conv1D\n",
    "from keras.layers.convolutional import MaxPooling1D\n",
    "from keras.layers.embeddings import Embedding\n",
    "from keras.preprocessing import sequence\n",
    "# fix random seed for reproducibility\n",
    "numpy.random.seed(7)\n",
    "# load the dataset but only keep the top n words, zero the rest\n",
    "top_words = 10000\n",
    "(X_train, y_train), (X_test, y_test) = imdb.load_data(num_words=top_words)\n",
    "# truncate and pad input sequences\n",
    "max_review_length = 500\n",
    "X_train = sequence.pad_sequences(X_train, maxlen=max_review_length)\n",
    "X_test = sequence.pad_sequences(X_test, maxlen=max_review_length)\n",
    "# create the model\n",
    "embedding_vecor_length = 32\n",
    "model = Sequential()\n",
    "model.add(Embedding(top_words, embedding_vecor_length, input_length=max_review_length))\n",
    "model.add(Conv1D(filters=32, kernel_size=3, padding='same', activation='relu'))\n",
    "model.add(MaxPooling1D(pool_size=2))\n",
    "model.add(LSTM(100))\n",
    "model.add(Dense(1, activation='sigmoid'))\n",
    "model.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
    "print(model.summary())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/2\n",
      "25000/25000 [==============================] - 165s 7ms/step - loss: 0.4496 - acc: 0.7674\n",
      "Epoch 2/2\n",
      "25000/25000 [==============================] - 165s 7ms/step - loss: 0.2255 - acc: 0.9132\n",
      "Accuracy: 88.13%\n"
     ]
    }
   ],
   "source": [
    "model.fit(X_train, y_train, epochs=2, batch_size=64)\n",
    "# Final evaluation of the model\n",
    "scores = model.evaluate(X_test, y_test, verbose=0)\n",
    "print(\"Accuracy: %.2f%%\" % (scores[1]*100))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There are a lot of opportunities for further optimization, such as the use of deeper and/or larger convolutional layers. However, do not make the model unnecessarily complex.\n",
    "\n",
    "You learned how to develop deep learning models for sentiment analysis including:\n",
    "\n",
    "- How to load and review the IMDB dataset within Keras.\n",
    "- How to develop a large neural network model for sentiment analysis.\n",
    "- How to develop a one-dimensional convolutional neural network model for sentiment analysis."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Summary\n",
    "\n",
    "- Keras provides high-level building blocks for developing deep learning models. \n",
    "- It doesn't handle low-level operations such as tensor manipulations and differentiation.\n",
    "- Instead, it relies on TensorFlow or Theano as the \"backend engine\".\n",
    "- So far, it is the most promising high-level library for Deep Learning!\n",
    "- Congratulations!"
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
