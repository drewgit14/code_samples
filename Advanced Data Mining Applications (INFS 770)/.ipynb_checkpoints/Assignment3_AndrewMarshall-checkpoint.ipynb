{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Andrew Marshall\n",
    "INFS 770\n",
    "Assignment 3\n",
    "04/08/2019\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Built-in/Generic Imports\n",
    "import os\n",
    "import sys\n",
    "#\n",
    "\n",
    "# Libraries\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import re\n",
    "import nltk\n",
    "import gensim\n",
    "#nltk.download('stopwords')\n",
    "#nltk.download('punkt')\n",
    "#nltk.download('wordnet')\n",
    "\n",
    "#import matplotlib.pyplot as plt\n",
    "#import mglearn.plots\n",
    "#\n",
    "\n",
    "# Modules\n",
    "from sklearn.feature_extraction.text import CountVectorizer, TfidfVectorizer, TfidfTransformer\n",
    "from nltk import word_tokenize \n",
    "from nltk.stem import WordNetLemmatizer \n",
    "from nltk.corpus import stopwords\n",
    "from gensim.models import LdaModel, LsiModel\n",
    "from sklearn.decomposition import TruncatedSVD\n",
    "#\n",
    "\n",
    "#Suppress scientific notation\n",
    "np.set_printoptions(suppress=True)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "T1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#import file for dataset\n",
    "file = open(\"reviews.txt\",\"r\") \n",
    "docs = [line.rstrip('\\n') for line in open(\"reviews.txt\",\"r\") ]\n",
    "docs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "T2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#preprocessing\n",
    "def before_token(documents):\n",
    "    # convert words to lower case\n",
    "    lower = map(str.lower, documents)\n",
    "    # remove puntuations\n",
    "    punctuationless = list(map(lambda x: \" \".join(re.findall('\\\\b\\\\w\\\\w+\\\\b',x)), lower))\n",
    "    # remove numbers\n",
    "    return list(map(lambda x:re.sub('\\\\b[0-9]+\\\\b', '', x), punctuationless))\n",
    "\n",
    "docs1 = before_token(docs)\n",
    "\n",
    "class LemmaTokenizer(object):\n",
    "    def __init__(self):\n",
    "        self.wnl = WordNetLemmatizer()\n",
    "    def __call__(self, doc):\n",
    "        return [self.wnl.lemmatize(t,\"v\") for t in word_tokenize(doc)]\n",
    "stopwords = nltk.corpus.stopwords.words(\"english\")\n",
    "vectorizer = TfidfVectorizer(tokenizer=LemmaTokenizer(),norm=None,stop_words=stopwords)\n",
    "# tokenize each document\n",
    "tokenized = map(nltk.word_tokenize, docs1)\n",
    "# pos tag each document\n",
    "tagged = map(nltk.pos_tag, tokenized)\n",
    "corpus_vect = vectorizer.fit_transform(docs1)\n",
    "#print(corpus_vect) # sparse matrix\n",
    "df_vect = pd.DataFrame(corpus_vect.toarray(), columns=vectorizer.get_feature_names())\n",
    "#print(df_vect)\n",
    "\n",
    "# remove stopwords\n",
    "# stopwords can only be removed after POS tags are generated. Otherwise, it will influence the POS tagging results.\n",
    "#stopwords = nltk.corpus.stopwords.words(\"english\")\n",
    "def remove_stopwords(doc):\n",
    "    out = []\n",
    "    for word in doc:\n",
    "        if word[0] not in stopwords: out.append(word)\n",
    "    return out\n",
    "\n",
    "no_stopwords = list(map(remove_stopwords, tagged))\n",
    "\n",
    "\n",
    "#print(no_stopwords)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# examine the mapping of words to feature indexes\n",
    "vectorizer.vocabulary_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# initialize a container of token frequencies\n",
    "fdist = nltk.FreqDist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# calculate the token frequency\n",
    "# the FreqDist function takes in a list of tokens and return a dict containg unique tokens and frequency\n",
    "fdist = nltk.FreqDist([token for doc in no_stopwords for token in doc])\n",
    "fdist"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Unique tokens: %d\" % fdist.B())\n",
    "print(\"Total tokens: %d\" % fdist.N())\n",
    "print(\"Tokens occurred only once: %d\" % len(fdist.hapaxes()))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "T3"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "TFIDF helps determine the numeric weight of words by using term frequency, document frequency, the number of documents as a function of evaluating the relative importance of words in a document or documents."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "T4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Different corpus_vect variables for the multiple runs\n",
    "corpus_vect1 = vectorizer.fit_transform(docs1)\n",
    "# convert the vectorized data to a gensim corpus object - corpus_vect1\n",
    "corpus1 = gensim.matutils.Sparse2Corpus(corpus_vect1, documents_columns=False)\n",
    "id2word = dict((v,k) for k,v in vectorizer.vocabulary_.items())\n",
    "print(id2word)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# build the lda model - lda1\n",
    "lda1 = LdaModel(corpus1, num_topics=3,id2word=id2word, passes=10)\n",
    "print(lda1.print_topics())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "T5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Different corpus_vect variables for the multiple runs\n",
    "corpus_vect2 = vectorizer.fit_transform(docs1)\n",
    "# convert the vectorized data to a gensim corpus object - corpus_vect2\n",
    "corpus2 = gensim.matutils.Sparse2Corpus(corpus_vect2, documents_columns=False)\n",
    "id2word2 = dict((v,k) for k,v in vectorizer.vocabulary_.items())\n",
    "#print(id2word2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# build the lda model - lda2\n",
    "lda2 = LdaModel(corpus2, num_topics=4,id2word=id2word2, passes=10)\n",
    "print(lda2.print_topics())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "T6"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Different corpus_vect variables for the multiple runs\n",
    "corpus_vect3 = vectorizer.fit_transform(docs1)\n",
    "# convert the vectorized data to a gensim corpus object - corpus_vect4\n",
    "corpus3 = gensim.matutils.Sparse2Corpus(corpus_vect3, documents_columns=False)\n",
    "id2word3 = dict((v,k) for k,v in vectorizer.vocabulary_.items())\n",
    "print(id2word3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# build the lda model-lda3\n",
    "lda3 = LdaModel(corpus3, num_topics=5,id2word=id2word3, passes=10)\n",
    "print(lda3.print_topics())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "T7"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Since T5 resulted in the largest sample, it can be determined using 5 topics will result in the best topic modeling results."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "T8"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vectorizer = TfidfVectorizer(tokenizer=LemmaTokenizer(),norm=\"l1\",stop_words=stopwords)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Different corpus_vect variables for the multiple runs\n",
    "corpus_vect4 = vectorizer.fit_transform(docs1)\n",
    "# convert the vectorized data to a gensim corpus object - corpus_vect4\n",
    "corpus4 = gensim.matutils.Sparse2Corpus(corpus_vect4, documents_columns=False)\n",
    "id2word4 = dict((v,k) for k,v in vectorizer.vocabulary_.items())\n",
    "print(id2word4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# build the lda model\n",
    "lda4 = LdaModel(corpus4, num_topics=5,id2word=id2word4, passes=10)\n",
    "print(lda4.print_topics())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "T9"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vectorizer = TfidfVectorizer(tokenizer=LemmaTokenizer(),norm=\"l2\",stop_words=stopwords)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Different corpus_vect variables for the multiple runs\n",
    "corpus_vect5 = vectorizer.fit_transform(docs1)\n",
    "# convert the vectorized data to a gensim corpus object - corpus_vect4\n",
    "corpus5 = gensim.matutils.Sparse2Corpus(corpus_vect5, documents_columns=False)\n",
    "id2word5 = dict((v,k) for k,v in vectorizer.vocabulary_.items())\n",
    "print(id2word5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# build the lda model\n",
    "lda5 = LdaModel(corpus5, num_topics=5,id2word=id2word5, passes=10)\n",
    "print(lda5.print_topics())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "T10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vectorizer = TfidfVectorizer(tokenizer=LemmaTokenizer(),stop_words=stopwords)\n",
    "#Different corpus_vect variables for the multiple runs\n",
    "corpus_vect10 = vectorizer.fit_transform(docs1)\n",
    "# convert the vectorized data to a gensim corpus object - corpus_vect4\n",
    "corpus10 = gensim.matutils.Sparse2Corpus(corpus_vect10, documents_columns=False)\n",
    "id2word = dict((v,k) for k,v in vectorizer.vocabulary_.items())\n",
    "#print(id2word)\n",
    "# build the lda model-lda3\n",
    "lda10 = LdaModel(corpus10, num_topics=5,id2word=id2word, passes=10)\n",
    "#print(lda10.print_topics())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# use the lda model to transform documents - norm = None\n",
    "lda10_docs = lda10[corpus10]\n",
    "#for row in lda10_docs:\n",
    "#    print(row)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# extract the scores and round them to 3 decimal places - lda10\n",
    "scores10 = np.round([[doc[1] for doc in row] for row in lda10_docs], 3)\n",
    "#print(scores10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# convert the documents scores into a data frame\n",
    "df_lda10 = pd.DataFrame(scores10, columns=[\"topic 1\", \"topic 2\",\"topic 3\",\"topic 4\",\"topic 5\"])\n",
    "df_lda10.head()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vectorizer = TfidfVectorizer(tokenizer=LemmaTokenizer(),norm=\"l1\",stop_words=stopwords)\n",
    "#Different corpus_vect variables for the multiple runs\n",
    "corpus_vect101 = vectorizer.fit_transform(docs1)\n",
    "# convert the vectorized data to a gensim corpus object - corpus_vect4\n",
    "corpus101 = gensim.matutils.Sparse2Corpus(corpus_vect101, documents_columns=False)\n",
    "id2word = dict((v,k) for k,v in vectorizer.vocabulary_.items())\n",
    "#print(id2word)\n",
    "# build the lda model-lda3\n",
    "lda101= LdaModel(corpus101, num_topics=5,id2word=id2word, passes=10)\n",
    "#print(lda3.print_topics())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# use the lda model to transform documents - norm = None\n",
    "lda101_docs = lda101[corpus101]\n",
    "#for row in lda101_docs:\n",
    "#    print(row)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# extract the scores and round them to 3 decimal places\n",
    "scores101 = np.round([[doc[1] for doc in row] for row in lda101_docs], 3)\n",
    "#print(scores101)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# convert the documents scores into a data frame\n",
    "df_lda101 = pd.DataFrame(scores101, columns=[\"topic 1\", \"topic 2\",\"topic 3\",\"topic 4\",\"topic 5\"])\n",
    "df_lda101.head()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vectorizer = TfidfVectorizer(tokenizer=LemmaTokenizer(),norm=\"l2\",stop_words=stopwords)\n",
    "#Different corpus_vect variables for the multiple runs\n",
    "corpus_vect102 = vectorizer.fit_transform(docs1)\n",
    "# convert the vectorized data to a gensim corpus object - corpus_vect4\n",
    "corpus102 = gensim.matutils.Sparse2Corpus(corpus_vect102, documents_columns=False)\n",
    "id2word = dict((v,k) for k,v in vectorizer.vocabulary_.items())\n",
    "#print(id2word)\n",
    "# build the lda model-lda3\n",
    "lda102 = LdaModel(corpus102, num_topics=5,id2word=id2word, passes=10)\n",
    "#print(lda102.print_topics())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# use the lda model to transform documents - norm = None\n",
    "lda102_docs = lda102[corpus102]\n",
    "#for row in lda102_docs:\n",
    "#    print(row)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# extract the scores and round them to 3 decimal places\n",
    "scores102 = np.round([[doc[1] for doc in row] for row in lda102_docs], 3)\n",
    "#print(scores102)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# convert the documents scores into a data frame\n",
    "df_lda102 = pd.DataFrame(scores102, columns=[\"topic 1\", \"topic 2\",\"topic 3\",\"topic 4\",\"topic 5\"])\n",
    "df_lda102.head()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Based on the scores, it can be stated that the \"l2\" norm provides the best topics."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "T11"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# convert the documents scores into a data frame - Best option\n",
    "df_lda102 = pd.DataFrame(scores102, columns=[\"topic 1\", \"topic 2\",\"topic 3\",\"topic 4\",\"topic 5\"])\n",
    "df_lda102.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "T12"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.options.display.float_format = '{:20,.4f}'.format\n",
    "\n",
    "model = TruncatedSVD(n_components=5).fit(df_vect)\n",
    "svd_proj = model.transform(df_vect)\n",
    "svd_final = pd.DataFrame(svd_proj, columns=[\"topic 1\", \"topic 2\",\"topic 3\",\"topic 4\",\"topic 5\"])\n",
    "svd_final.head()\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
