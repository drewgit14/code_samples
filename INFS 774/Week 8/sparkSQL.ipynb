{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Spark SQL and DataFrames"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Spark SQL is a Spark module for structured data processing. Unlike the basic Spark RDD API, the interfaces provided by Spark SQL provide Spark with more information about the structure of both the data and the computation being performed. Internally, Spark SQL uses this extra information to perform extra optimizations."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Architecture of Spark SQL"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![Spark SQL Architecture](sparksql_arch.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Major components of SparkSQL\n",
    "- Datasets: A Dataset is a distributed collection of data. Dataset is a new interface added in Spark 1.6 that provides the benefits of RDDs (strong typing, ability to use powerful lambda functions) with the benefits of Spark SQLâ€™s optimized execution engine. A Dataset can be constructed from JVM objects and then manipulated using functional transformations (map, flatMap, filter, etc.). The Dataset API is available in Scala and Java. Python does not have the support for the Dataset API.\n",
    "- DataFrames: A DataFrame is a Dataset organized into named columns. It is conceptually equivalent to a table in a relational database or a data frame in R/Python, but with richer optimizations under the hood. DataFrames can be constructed from a wide array of sources such as: structured data files, tables in Hive, external databases, or existing RDDs. The DataFrame API is available in Scala, Java, Python, and R. \n",
    "- Catalyst Optimizer:  Catalyst optimizer enables several key features, such as query optimization and schema inference (from JSON data)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Starting Point: SparkSession\n",
    "SparkSession was introduced in Spark version 2. It is the entry point into all functionality in Spark. To create a basic SparkSession, just use SparkSession.builder:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "\n",
    "spark = SparkSession \\\n",
    "    .builder \\\n",
    "    .appName(\"Python Spark SQL basic example\") \\\n",
    "    .config(\"spark.some.config.option\", \"some-value\") \\\n",
    "    .enableHiveSupport() \\\n",
    "    .getOrCreate()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### How is SparkSession different from SparkContext we learned last time?\n",
    "Prior to Spark 2.0, SparkContext was the entry point for Spark applications with RDDs. For every other APIs, different contexts were required - For SQL, SQL Context was required; For Streaming, Streaming Context was required; For Hive, Hive Context was required. Starting from Apache Spark 2.0, Spark Session is the new entry point for all these Spark applications."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Creating DataFrames"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "With a SparkSession, applications can create DataFrames from an existing RDD, from a Hive table, or from Spark data sources such as json or csv files. As an example, the following creates a DataFrame based on the content of a JSON file:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+-------+\n",
      "|age|   name|\n",
      "+---+-------+\n",
      "| 22|   John|\n",
      "| 36| Andrew|\n",
      "| 22| Clarke|\n",
      "| 42|  Kevin|\n",
      "| 51|Richard|\n",
      "+---+-------+\n",
      "\n",
      "[Row(age=22, name='John'), Row(age=36, name='Andrew'), Row(age=22, name='Clarke'), Row(age=42, name='Kevin'), Row(age=51, name='Richard')]\n",
      "+---+-------+\n",
      "|age|   name|\n",
      "+---+-------+\n",
      "| 22|   John|\n",
      "| 36| Andrew|\n",
      "| 22| Clarke|\n",
      "| 42|  Kevin|\n",
      "| 51|Richard|\n",
      "+---+-------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# spark is an existing SparkSession\n",
    "df = spark.read.json(\"people.json\")\n",
    "# Displays the content of the DataFrame in a tabular format to stdout\n",
    "df.show()\n",
    "# last time, we learned the method collect(), which also works here, but it will output a list of rows.\n",
    "print(df.collect())\n",
    "# you can also sqlContext to create a DataFrame\n",
    "dfSQL = sqlContext.read.json(\"people.json\")\n",
    "dfSQL.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## DataFrame Operations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- age: long (nullable = true)\n",
      " |-- name: string (nullable = true)\n",
      "\n",
      "+-------+\n",
      "|   name|\n",
      "+-------+\n",
      "|   John|\n",
      "| Andrew|\n",
      "| Clarke|\n",
      "|  Kevin|\n",
      "|Richard|\n",
      "+-------+\n",
      "\n",
      "+-------+---------+\n",
      "|   name|(age + 1)|\n",
      "+-------+---------+\n",
      "|   John|       23|\n",
      "| Andrew|       37|\n",
      "| Clarke|       23|\n",
      "|  Kevin|       43|\n",
      "|Richard|       52|\n",
      "+-------+---------+\n",
      "\n",
      "+---+-------+\n",
      "|age|   name|\n",
      "+---+-------+\n",
      "| 22|   John|\n",
      "| 36| Andrew|\n",
      "| 22| Clarke|\n",
      "| 42|  Kevin|\n",
      "| 51|Richard|\n",
      "+---+-------+\n",
      "\n",
      "+---+-----+\n",
      "|age|count|\n",
      "+---+-----+\n",
      "| 22|    2|\n",
      "| 51|    1|\n",
      "| 36|    1|\n",
      "| 42|    1|\n",
      "+---+-----+\n",
      "\n",
      "<class 'pyspark.sql.dataframe.DataFrame'>\n"
     ]
    }
   ],
   "source": [
    "# spark, df are from the previous example\n",
    "# Print the schema in a tree format\n",
    "df.printSchema()\n",
    "\n",
    "# Select only the \"name\" column\n",
    "df.select(\"name\").show()\n",
    "\n",
    "# Select everybody, but increment the age by 1\n",
    "df.select(df['name'], df['age'] + 1).show()\n",
    "\n",
    "# Select people older than 21\n",
    "df.filter(df['age'] > 21).show()\n",
    "\n",
    "# Count people by age\n",
    "df.groupBy(\"age\").count().show()\n",
    "\n",
    "# we are doing dataframe transformations.\n",
    "print(type(df.groupBy(\"age\").count()))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There are many functions defined for DataFrame, you can refer to https://spark.apache.org/docs/2.1.0/api/python/pyspark.sql.html#pyspark.sql.DataFrame for the complete list of functions."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "DataFrames also have a rich library of functions including string manipulation, date arithmetic, common math operations and more. The complete list is available in https://spark.apache.org/docs/2.1.0/api/python/pyspark.sql.html#module-pyspark.sql.functions. Below is the example for computing the mean of ages. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Row(avg(age)=34.6)]"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from pyspark.sql import functions as F\n",
    "df.agg(F.avg(df['age'])).collect()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Running SQL Queries Programmatically"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+-------+\n",
      "|age|   name|\n",
      "+---+-------+\n",
      "| 22|   John|\n",
      "| 36| Andrew|\n",
      "| 22| Clarke|\n",
      "| 42|  Kevin|\n",
      "| 51|Richard|\n",
      "+---+-------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Register the DataFrame as a SQL temporary view\n",
    "df.createOrReplaceTempView(\"people\")\n",
    "\n",
    "sqlDF = spark.sql(\"SELECT * FROM people\")\n",
    "sqlDF.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Global Temporary View\n",
    "Temporary views in Spark SQL are session-scoped and will disappear if the session that creates it terminates. If you want to have a temporary view that is shared among all sessions and keep alive until the Spark application terminates, you can create a global temporary view. Global temporary view is tied to a system preserved database global_temp, and we must use the qualified name to refer it, e.g. SELECT * FROM global_temp.view1."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+-------+\n",
      "|age|   name|\n",
      "+---+-------+\n",
      "| 22|   John|\n",
      "| 36| Andrew|\n",
      "| 22| Clarke|\n",
      "| 42|  Kevin|\n",
      "| 51|Richard|\n",
      "+---+-------+\n",
      "\n",
      "+---+-------+\n",
      "|age|   name|\n",
      "+---+-------+\n",
      "| 22|   John|\n",
      "| 36| Andrew|\n",
      "| 22| Clarke|\n",
      "| 42|  Kevin|\n",
      "| 51|Richard|\n",
      "+---+-------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Register the DataFrame as a global temporary view\n",
    "df.createOrReplaceGlobalTempView(\"people\")\n",
    "\n",
    "# Global temporary view is tied to a system preserved database `global_temp`\n",
    "spark.sql(\"SELECT * FROM global_temp.people\").show()\n",
    "\n",
    "# Global temporary view is cross-session\n",
    "spark.newSession().sql(\"SELECT * FROM global_temp.people\").show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Interoperating with RDDs\n",
    "Spark SQL supports two different methods for converting existing RDDs into DataFrames/Datasets. The first method uses reflection to infer the schema of an RDD that contains specific types of objects. This reflection-based approach leads to more concise code and works well when you already know the schema while writing your Spark application.\n",
    "\n",
    "The second method for creating DataFrames/Datasets is through a programmatic interface that allows you to construct a schema and then apply it to an existing RDD. While this method is more verbose, it allows you to construct Datasets when the columns and their types are not known until runtime."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Inferring the Schema Using Reflection\n",
    "Spark SQL can convert an RDD of Row objects to a DataFrame, inferring the datatypes. Rows are constructed by passing a list of key/value pairs as kwargs to the Row class. The keys of this list define the column names of the table, and the types are inferred by sampling the whole dataset, similar to the inference that is performed on JSON files."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[['John', '22'], ['Andrew', '36'], ['Clarke', '22'], ['Kevin', '42'], ['Richard', '51']]\n",
      "[Row(age=22, name='John'), Row(age=36, name='Andrew'), Row(age=22, name='Clarke'), Row(age=42, name='Kevin'), Row(age=51, name='Richard')]\n",
      "root\n",
      " |-- age: long (nullable = true)\n",
      " |-- name: string (nullable = true)\n",
      "\n",
      "[Row(age=22, name='John'), Row(age=22, name='Clarke')]\n",
      "Name: John\n",
      "Name: Clarke\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql import Row\n",
    "\n",
    "sc = spark.sparkContext\n",
    "\n",
    "# Load a text file and convert each line to a Row.\n",
    "lines = sc.textFile(\"people.txt\")\n",
    "parts = lines.map(lambda l: l.split(\",\"))\n",
    "print(parts.collect())\n",
    "people = parts.map(lambda p: Row(name=p[0], age=int(p[1])))\n",
    "print(people.collect())\n",
    "\n",
    "# Infer the schema, and register the DataFrame as a table.\n",
    "schemaPeople = spark.createDataFrame(people)\n",
    "schemaPeople.printSchema()\n",
    "schemaPeople.createOrReplaceTempView(\"people\")\n",
    "\n",
    "# SQL can be run over DataFrames that have been registered as a table.\n",
    "youths = spark.sql(\"SELECT * FROM people WHERE age >= 13 AND age <= 30\")\n",
    "\n",
    "# The results of SQL queries are Dataframe objects.\n",
    "# rdd returns the content as an :class:`pyspark.RDD` of :class:`Row`.\n",
    "print(youths.rdd.collect())\n",
    "someNames = youths.rdd.map(lambda p: \"Name: \" + p['name']).collect()\n",
    "for name in someNames:\n",
    "    print(name)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Programmatically Specifying the Schema"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "When a dictionary of kwargs cannot be defined ahead of time (for example, the structure of records is encoded in a string, or a text dataset will be parsed and fields will be projected differently for different users), a DataFrame can be created programmatically with three steps.\n",
    "\n",
    "1. Create an RDD of tuples or lists from the original RDD;\n",
    "2. Create the schema represented by a StructType matching the structure of tuples or lists in the RDD created in the step 1.\n",
    "3. Apply the schema to the RDD via createDataFrame method provided by SparkSession.\n",
    "For example:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[StructField(name,StringType,true), StructField(age,StringType,true)]\n",
      "root\n",
      " |-- name: string (nullable = true)\n",
      " |-- age: string (nullable = true)\n",
      "\n",
      "+-------+\n",
      "|   name|\n",
      "+-------+\n",
      "|   John|\n",
      "| Andrew|\n",
      "| Clarke|\n",
      "|  Kevin|\n",
      "|Richard|\n",
      "+-------+\n",
      "\n",
      "root\n",
      " |-- name: string (nullable = true)\n",
      " |-- age: integer (nullable = true)\n",
      "\n",
      "+-------+---+\n",
      "|   name|age|\n",
      "+-------+---+\n",
      "|   John| 22|\n",
      "| Andrew| 36|\n",
      "| Clarke| 22|\n",
      "|  Kevin| 42|\n",
      "|Richard| 51|\n",
      "+-------+---+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Import data types\n",
    "from pyspark.sql.types import *\n",
    "\n",
    "sc = spark.sparkContext\n",
    "\n",
    "# Load a text file and convert each line to a Row.\n",
    "lines = sc.textFile(\"people.txt\")\n",
    "parts = lines.map(lambda l: l.split(\",\"))\n",
    "# Each line is converted to a tuple.\n",
    "people = parts.map(lambda p: (p[0], p[1].strip()))\n",
    "\n",
    "# The schema is encoded in a string.\n",
    "schemaString = \"name age\"\n",
    "\n",
    "fields = [StructField(field_name, StringType(), True) for field_name in schemaString.split()]\n",
    "print(fields)\n",
    "\n",
    "schema = StructType(fields)\n",
    "# Apply the schema to the RDD.\n",
    "schemaPeople = spark.createDataFrame(people, schema)\n",
    "schemaPeople.printSchema()\n",
    "\n",
    "# Creates a temporary view using the DataFrame\n",
    "schemaPeople.createOrReplaceTempView(\"people\")\n",
    "\n",
    "# SQL can be run over DataFrames that have been registered as a table.\n",
    "results = spark.sql(\"SELECT name FROM people\")\n",
    "results.show()\n",
    "\n",
    "# how to change \"age\" to integer? simple:\n",
    "df = spark.sql(\"select name, int(age) as age from people\")\n",
    "df.printSchema()\n",
    "df.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## How to read CSV file?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Generic Load Functions \n",
    "You can also manually specify the data source that will be used along with any extra options that you would like to pass to the data source. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+---+\n",
      "|   name|age|\n",
      "+-------+---+\n",
      "|   John| 22|\n",
      "| Andrew| 36|\n",
      "| Clarke| 22|\n",
      "|  Kevin| 42|\n",
      "|Richard| 51|\n",
      "+-------+---+\n",
      "\n",
      "root\n",
      " |-- name: string (nullable = true)\n",
      " |-- age: integer (nullable = true)\n",
      "\n",
      "+-------+---+\n",
      "|   name|age|\n",
      "+-------+---+\n",
      "|   John| 22|\n",
      "| Andrew| 36|\n",
      "| Clarke| 22|\n",
      "|  Kevin| 42|\n",
      "|Richard| 51|\n",
      "+-------+---+\n",
      "\n",
      "root\n",
      " |-- name: string (nullable = true)\n",
      " |-- age: integer (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# if the CSV file has a header\n",
    "df = spark.read.load(\"people.csv\",format=\"csv\", sep=\",\", inferSchema=\"true\", header=\"true\")\n",
    "df.show()\n",
    "df.printSchema()\n",
    "# if the CSV file does not have a header\n",
    "from pyspark.sql.types import StructType, StructField, IntegerType, StringType\n",
    "schema = StructType([\n",
    "    StructField(\"name\", StringType(), True),\n",
    "    StructField(\"age\", IntegerType(), True)])\n",
    "df = spark.read.load(\"people2.csv\",format=\"csv\", sep=\",\", header=\"false\", schema=schema)\n",
    "df.show()\n",
    "df.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+-------+\n",
      "|age|   name|\n",
      "+---+-------+\n",
      "| 22|   John|\n",
      "| 36| Andrew|\n",
      "| 22| Clarke|\n",
      "| 42|  Kevin|\n",
      "| 51|Richard|\n",
      "+---+-------+\n",
      "\n",
      "root\n",
      " |-- age: long (nullable = true)\n",
      " |-- name: string (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# you can also use the generic load function to read other types of files. For example:\n",
    "df = spark.read.load(\"people.json\", format=\"json\")\n",
    "df.show()\n",
    "df.printSchema()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
