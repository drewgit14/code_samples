---
title: "Homework 5 1,3,5,6"
author: "Andrew Marshall"
date: "6/28/2018"
output:
  word_document: default
  pdf_document: default
  html_document: default
---

# Data

As with Homework 4, all the numeric values you need, other than `0.05`, `0`, `1`, `2` and `3` are defined below:

```{r}
Year=c(1936, 1946, 1951, 1963, 1975, 1997, 2006)
CaloriesPerRecipeMean <- c(2123.8, 2122.3, 2089.9, 2250.0, 2234.2, 2249.6, 3051.9)
CaloriesPerRecipeSD <- c(1050.0, 1002.3, 1009.6, 1078.6, 1089.2, 1094.8, 1496.2)
CaloriesPerServingMean <- c(268.1, 271.1, 280.9, 294.7, 285.6, 288.6, 384.4)
CaloriesPerServingSD <- c(124.8, 124.2, 116.2, 117.7, 118.3, 122.0, 168.3)
ServingsPerRecipeMean <- c(12.9, 12.9, 13.0, 12.7, 12.4, 12.4, 12.7)
ServingsPerRecipeSD <- c(13.3, 13.3, 14.5, 14.6, 14.3, 14.3, 13.0)

CookingTooMuch.dat <- data.frame(
  Year=Year,
  CaloriesPerRecipeMean = CaloriesPerRecipeMean,
  CaloriesPerRecipeSD = CaloriesPerRecipeSD,
  CaloriesPerServingMean = CaloriesPerServingMean,
  CaloriesPerServingSD = CaloriesPerServingSD,
  ServingsPerRecipeMean = ServingsPerRecipeMean,
  ServingsPerRecipeSD = ServingsPerRecipeSD
)

sample.size <- 18
tenth.increment <- 0.10
hundredth.increment <- 0.100
idx.1936 <- 1
idx.2006 <- length(CaloriesPerRecipeMean)
idxs36_07 <- c(idx.1936,idx.2006)
alpha=0.05
```

Nearly all the same restrictions apply. Specifically


* There are 6 exercises. Choose 4 to be graded.
* One of the exercises must be completed in both SAS and R. Make sure you document this in the output.
* The other 3 exercises are to be complete in either R or SAS. Make sure you document this in the output.
* You may choose to work the other exercises. If you do, put these *after* the exercises you want graded. Otherwise, we'll grade the first four exercises and stop grading there. Time permitting, we'll provide feedback on the additional exercises.
* You are not required to write additional (other than previous Homework) functions for this exercise, but you may. You will be expected to clearly document additional functions - identify the expected inputs and outputs.

* There are no unit tests for this exercise. Where applicable, you should compare your results to comparable results in previous homework.

* This is an exercise in working with data tables. For three exercises, you will create data tables from sequences; for the last two you are expected to read data from files. One requires you to convert a data table to matrices, without calling `data.frame` directly.

* If you choose SAS, some of the exercises will require you to transfer data between PROC IML and the DATA step. You may need to redefine macros from previous assignments. 


# Exercise 1

Repeat the analysis from Exercise 1, Homework 4. This time, the results wil be in a data table with 49 rows. There will be 7 columns in the final table, `Year1`, `Year2`, `Mean1`, `SD1`, `Mean2`, `SD2` and `CohenD`. This table will have the same duplications as your matrix in Homework 4 (don't worry, we'll remove those in later exercises).

## Part a

Create a data table where each row represents a different combination between years. It should look something like:

Year1 | Year2 |  Mean1 |  Mean2 |    SD1 |    SD2 
------|-------|--------|--------|--------|-------
 1936 |  1936 | 2123.8 | 2123.8 | 1050.0 | 1050.0
 1946 |  1936 | 2122.3 | 2123.8 | 1002.3 | 1050.0
 1951 |  1936 | 2089.9 | 2123.8 | 1009.6 | 1050.0
 1963 |  1936 | 2250.0 | 2123.8 | 1078.6 | 1050.0
 ...  |   ... |    ... |    ... |    ... |    ...     
 1936 |  1946 | 2123.8 | 2122.3 | 1050.0 | 1002.3
 1946 |  1946 | 2122.3 | 2122.3 | 1002.3 | 1002.3
 ...  |   ... |    ... |    ... |    ... |    ... 


Start with the vectors defined in Data. You can reuse the matrices from the last homework, if you wish, or you can create new sequences while constructing the data table

If you do this exercise in SAS, use IML to create the vectors, the use `CREATE` to create a data table.
```{r}


#Initial Data frame setup
Year1.df <- data.frame(
 Year1 =  rep(CookingTooMuch.dat$Year[1:7],idx.2006)
)
Year2.df <- data.frame(
 Year2 = rep(CookingTooMuch.dat$Year[1:7],each=idx.2006)
)
Mean1.df <- data.frame(
  Mean1 = rep(CookingTooMuch.dat$CaloriesPerRecipeMean[1:7],idx.2006)
)
Mean2.df <- data.frame(
  Mean2 = rep(CookingTooMuch.dat$CaloriesPerRecipeMean[1:7],each=idx.2006)
)
SD1.df <- data.frame(
   SD1 = rep(CookingTooMuch.dat$CaloriesPerRecipeSD[1:7],idx.2006)
)
SD2.df <- data.frame(
  SD2 = rep(CookingTooMuch.dat$CaloriesPerRecipeSD[1:7],each=idx.2006)

)

#Final data frame setup
CPR.df <- data.frame(
  Year1.df,
  Year2.df,
  Mean1.df,
  Mean2.df,
  SD1.df,
  SD2.df
  )

CPR.df





    
  
  












```

## Part b.

Below is a wrapper function that accepts a vector as a parameter and returns Cohen's $d$ for values in that vector. Modify this function so that it selects appropriate elements from the vector and calls **your* Cohen's $d$ function from the previous homework. Assume that `table.row` is a row from the data table you created in Part a.


```{r}
# cohen.wrapper <- function(table.row) {
#   return(cohen.d(table.row[3],table.row[5],table.row[4],table.row[6]))

#Cohen's d function for use with Cohen Wrapper
d_12 <- function(m_1,m_2,s_1,s_2) {
d_12_var <- (abs(m_1-m_2))/(sqrt((s_1^2 + s_2^2)/2))
return(d_12_var)
}


cohen.wrapper <- function(table.row) {
    return(d_12(table.row[3],table.row[4],table.row[5],table.row[6]))
}

CohenD.df <- data.frame(
  cohen.wrapper(CPR.df) 
)
colnames(CohenD.df) <- c("CohenD")

CohenD.df





```

If you choose SAS for this exercise, define a macro to implement Cohen's $d$ formula, using syntax compatible with IML. This macro should have four parameters appropriate for Cohen's $d$.

## Part c

Compute `CohenD` and add this to your table using `apply` and the wrapper function in Part b. Print this table and compare to the matrix you produced in Homework 4.

If you choose SAS, create a second data table, starting with the data table in part a (use SET in the data statement). Insert your macro in the body of this data step. Print your table. Use the names of your data table as parameters to this macro. Your macro will be replaced with the formula; assign the macro invocation a data variable `CohenD`.
```{r}

# using Apply function to compute CohenD column
CohenD <- apply(CPR.df,1,cohen.wrapper)

#Adding new column to data frame
cbind(CPR.df,CohenD)

```
# Exercise 2

In this exercise, we reproduce and extend the plot from Exercise 2, Homework 4. 

## Part a
Create a data table with a sequence from `x3 =` $\mu-3\sigma \dots \mu+3\sigma$ using increments defined by `tenth.increment`, as before. Name this column `X`. 

```{r}
mu <- 0
sigma <- 1

 # Generating sequences for x3.df data frames
 x3 <- seq(from=(mu-3*sigma),to=(mu+3*sigma),by=tenth.increment)

x3.df <- data.frame(
  x = x3
)
x3.df






```


If you choose SAS, do this step in IML.

## Part b

Compute the likelihood and assign this to three columns, `L1`, `L2`, `L3`. These columns will correspond to the values computed for sequences x1, x2 and x3. To make columns `L1` and `L2` fit in this data table, pad the columns with NA values. 

X                   |                L1 |                L2 |  L3  
--------------------|-------------------|-------------------|-------
$\mu-3\sigma$       |                 - |                 - | $L(x,\mu,\sigma)$
$(\mu-3\sigma)+0.1$ |                 - |                 - | $L(x,\mu,\sigma)$
$(\mu-3\sigma)+0.2$ |                 - |                 - | $L(x,\mu,\sigma)$
...                 |               ... |               ... |    ... |     
$\mu-2\sigma$       |                 - | $L(x,\mu,\sigma)$ | $L(x,\mu,\sigma)$
$(\mu-2\sigma)+0.1$ |                 - | $L(x,\mu,\sigma)$ |  $L(x,\mu,\sigma)$
$(\mu-2\sigma)+0.2$ |                 - | $L(x,\mu,\sigma)$ | $L(x,\mu,\sigma)$
...                 |               ... |    ... |    ... |    ... |    ... 
$\mu-\sigma$        | $L(x,\mu,\sigma)$ | $L(x,\mu,\sigma)$ | $L(x,\mu,\sigma)$
$(\mu-\sigma)+0.1$  |$L(x,\mu,\sigma)$ | $L(x,\mu,\sigma)$ |  $L(x,\mu,\sigma)$
$(\mu-\sigma)+0.2$  | $L(x,\mu,\sigma)$ | $L(x,\mu,\sigma)$ | $L(x,\mu,\sigma)$
...                 |   ... |    ... |    ... |    ... |    ... 
$(\mu+3\sigma)-0.1$ |  - | - |  $L(x,\mu,\sigma)$
$\mu+3\sigma$       |  - | - | $L(x,\mu,\sigma)$

A couple different approaches you might try:

- Use `L3` values for all columns, but change values to `NA` outside the appropriate range (you can use boolean indexes in R).
- Reuse the sequences from Homework 4 to build the data frame, concatenating with sequences of `NA` as needed.

If you choose SAS, costruct this data in IML, then use `CREATE` to create a data table. 

```{r}

 # Generating sequences for x1.df,x2.df data frames
 x1 <- seq(from=(mu-sigma),to=(mu+sigma),by=tenth.increment)
 x2 <- seq(from=(mu-2*sigma),to=(mu+2*sigma),by=tenth.increment)




# assigning values for use with data frame setup
l1 <- (1/(sigma * sqrt(2 * pi))) * (exp(-((x1-mu)^2)/(2*sigma^2)))
l2 <- (1/(sigma * sqrt(2 * pi))) * (exp(-((x2-mu)^2)/(2*sigma^2))) 
l3 <- (1/(sigma * sqrt(2 * pi))) * (exp(-((x3-mu)^2)/(2*sigma^2)))


# Defining L2 and L2 data frames
L1.df <- data.frame(
  L1 = l1
)

L2.df <- data.frame(
  L2 = l2
)
L2.df

L3.df <- data.frame (
  L3 =  l3
)

 #Creating matrix for transition to data frame
 L.mat <-matrix(nrow = 61,ncol = 4)
 L.mat[1:61,1] <- x3.df[1:61,1]
 L.mat[1:61,4] <- L3.df[1:61,1]
 L.mat[1:61,3] <- L2.df[1:61,1]
 L.mat[1:61,2] <- L1.df[1:61,1]
 
 
 #Assigning matrix to data frame
 L.df <- data.frame(
   L.mat
 )
 
 L.df
 























```


## Part c

Plot `L3` vs `X`, using formula syntax, with this data table as a parameter to plot, using points as the symbol. 

Add points for `L2`, then `L1`, using different colors or symbols. Complete the plot by adding vertical lines at $\pm 1$ and $\pm 2$, using colors matching `L1` and `L2`, respectively.

If you use SAS, you will only need to call one `SGPLOT` block, with multiple `series` or `scatter` statements.

If you follow the instructions, you should have a graph of a normal probability distribution with different colors for the parts of the curve representing 1, 2 and 3 standard deviations.

```{r}
#Assignments for L3 and x3 plots
x <- c(L.df$X1)
y <- c(L.df$X4)

#Assignments for L2 and L1 plots
y2 <- c(L.df$X3)
y1 <- c(L.df$X2)

# L1, L2, and L3 plots vs x
plot(x,y,main = "Exercise 2, part c",type="l")
points(x,y2,col="blue",type="p")
points(x,y1,col="red",type="l")


```




## Part d
Use `apply` (or similar function), compute the sum of columns L1, L2 and L3, multiplied by `tenth.increment`. Compare these values with the sums calculated for the previous exercise.

```{r}

sum(L1.df,L2.df,L3.df) * tenth.increment


```


If you use SAS, you can use PROC SUMMARY for this step.


# Exercise 3

Starting with 'CookingToMuch.dat', repeat the analysis from Homework 4, Exercise 5. 

Append an appropriate column (named `Intercept`), with all values of 1, to `CookingToMuch.dat`, then use column indexes to extract appropriate $X$ and $y$ variables from `CookingToMuch.dat`. Do not create new matrices or frames for $X$ and $y$. You may to coerce $X$ or $Y$ to matrices.

If you use SAS for this exercise, use the data table `CookingToMuch`, and use IML functions (`USE/READ`) to read from this data table into matrices.


Compute and print `beta.hat` as before, and compare to
```{r}
lm(CaloriesPerRecipeMean ~ CaloriesPerServingMean,data=CookingTooMuch.dat)
```

Change the eval flag in this for an alternative model.

```{r,eval=FALSE}
lm(CaloriesPerRecipeMean ~ 0 + Intercept +  CaloriesPerServingMean,data=CookingTooMuch.dat)
```


# Exercise 4

This exercise will be similar to Exercise 4 in Homework 4. 

## Part a

First, find the minimum and maximum values for ServingsPerRecipeMean.

## Part b

Using your Poisson confidence interval function from Homework 3, Exercise 4, calculate the lower and upper bounds to the minimum and maximum means found in Part a. Set `LB` as the smallest (single value) of these bounds, set `UB` as the largest (single value) of these bounds.

## Part c

Create a data frame with a column 'Y' as a sequence from `floor(LB)` to `ceiling(UB)`. Add to this data frame two columns, one the Poisson probability from Homework 4 using
$\mu=$ the maximum servings per recipe mean, and the other using the minimum servings per recipe mean.

## Part d

Plot both probability series against `Y`, using different colors. Use formula notation for the plots.

Add to this plot two vertical lines, located at the minimum and maximum mean values. Use the same color as the corresponding probablity curves.

Finally, add two pairs of lines corresonding to the upper and lower CI of the minimum and maximum means, calculated in Part b. Use different line types for these lines.

If you do this exercise in SAS, you can do parts a-c in IML, saving the matrices to a data table and use SGPLOT for part d.


# Exercise 5

I was shopping for a motorcycle this spring, and in researching models, found a list of the fastest production motorcycles (https://en.wikipedia.org/wiki/List_of_fastest_production_motorcycles) . I edited this page to create a data table in CSV format.

## Part a

Download the file `fastest.csv` from D2L and read the file into a data frame or table. Print a summary of the table.

```{r}
PathToFastest = "C:/Users/drewm/Documents/GitHub/code-stat700/fastest.csv"
Fastest.dat <- read.csv(PathToFastest,header=TRUE)

summary(Fastest.dat)






```



## Part b

To show that the data was read correctly, create three plots. Plot

1. Make vs Engine 
2. Horsepower vs Engine
3. MPH vs Horsepower

These three plots should reproduce the three types of plots shown in the `RegressionEtcPlots` video, **Categorical vs Categorical**, **Continuous vs Continuous** and **Continuous vs Categorical**. Add these as titles to your plots, as appropriate.


```{r}
#Categorical vs Categorical plot
plot(main="Categorical vs Categorical",Make~Engine,data = Fastest.dat)

#Continous vs Categorical plot
plot(main = "Continous vs Categorical", Horsepower~Engine,data = Fastest.dat)

#Continuous vs Continous plot
plot(main = "Continuous vs Continous", MPH~Horsepower,data = Fastest.dat)



```




# Exercise 6

## Part a

Go to http://www.itl.nist.gov/div898/strd/anova/AtmWtAg.html and download the file listed under `Data File in Table Format` (ttps://www.itl.nist.gov/div898/strd/anova/AtmWtAgt.dat)

```{r}

#File has been downloaded as requested

```


## Part b

Edit this into a file that can be read into R or SAS, or find an appropriate function that can read the file as-is. You will need to upload this file to D2L along with your Rmd/SAS files. Provide a brief comment on changes you make, or assumptions about the file needed for you file to be read into R/SAS. Read the file into a data frame or data table.

```{r}

#Comment:  No file editing was done.  File was downloaded and the code provided access to the necessary file and data.

#Path information for reading file from local machine
PathToAtmWtAgt = "C:/Users/drewm/Documents/GitHub/code-stat700/AtmWtAgt.dat"

#Assigning data from file to data frame
AtmWtAgt.df <- read.delim(PathToAtmWtAgt,header=TRUE,skip= 57,sep = "",as.is=TRUE)

#Displaying data in dataframe
AtmWtAgt.df



```



## Part c

Calculate mean, sd and sample size for the two columns in this data; printing the results. You should store the values in variables. Use function(s) from Homework 3 to answer these two questions:

1. Is the difference between the two columns a small, medium or large effect size?
2. Is the difference between the two columns statistically significant?

Do this by printing function call(s) and results.

```{r}
#Cohen D function
d_12 <- function(m_1,m_2,s_1,s_2) {
  d_12_var <- (abs(m_1-m_2))/(sqrt((s_1^2 + s_2^2)/2))
  return(d_12_var)
}

#Fisher LSD funtion
fisher.lsd <- function(s_i, n_i, s_j, n_j, alpha=0.05) {
  s2 <- ((n_i-1)*s_i^2 + (n_j-1)*s_j^2) / ((n_i-1)+(n_j-1))
  critical.t <- qt(1 - alpha/2,n_i+n_j-2)
  return(critical.t*sqrt(s2*(1/n_i + 1/n_j)))
}

# Function for chi^2 calcuations

chisq.ci <- function(x,alpha=0.05) {
p_lower <- alpha/2
p_upper <- (1-alpha)/2
df_lower <- 2*x
df_upper <- 2*(x+1)
local.chisqlower <- qchisq(p_lower, df_lower, ncp = 0, lower.tail = TRUE, log.p = FALSE)/2
local.chisqupper <- qchisq(p_upper, df_upper, ncp = 0, lower.tail = TRUE, log.p = FALSE)
return(list(chisqlower = local.chisqlower,chisqupper = local.chisqupper))
}

# pwr.t.test(n = , d = , sig.level =0.05 , power =NULL , type = c("two.sample", "one.sample", "paired"))

#Mean, SD, and Sample Size for column 1
AtmWtAgt1.mean <- mean(AtmWtAgt.df$X1,na.rm = TRUE)
m_1<-AtmWtAgt1.mean
m_1
AtmWtAgt1.SD <- sd(AtmWtAgt.df$X1)
s_1<-AtmWtAgt1.SD
s_1
AtmWtAgt1.SampleSize <- chisq.ci(18,alpha = 0.05)
AtmWtAgt1.SampleSize

#Mean, SD and Sample Size for Column 2
AtmWtAgt2.mean <- mean(AtmWtAgt.df$X2,na.rm = TRUE)
m_2 <- AtmWtAgt2.mean
m_2
AtmWtAgt2.SD <- sd(AtmWtAgt.df$X2)
s_2<-AtmWtAgt2.SD
s_2
AtmWtAgt2.SampleSize <- chisq.ci(18,alpha = 0.05)
AtmWtAgt2.SampleSize

d_12(m_1,m_2,s_1,s_2)
fisher.lsd(s_1, sample.size , s_2, sample.size , alpha=0.05)

  
#Q1 Answer: small
#Q2 Answer: No








```






