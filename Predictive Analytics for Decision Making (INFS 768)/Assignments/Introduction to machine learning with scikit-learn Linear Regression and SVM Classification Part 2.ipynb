{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<p style=\"font-family: Arial; font-size:3.75em;color:purple; font-style:bold\"><br>\n",
    "Introduction to machine <br><br><br>learning with scikit-learn<br>\n",
    "<br><br>Week 3<br></p>\n",
    "\n",
    "<p style=\"font-family: Arial; font-size:1.75em;color:#2462C0; font-style:bold\"><br>\n",
    "Basic concept of machine learning</p>\n",
    "\n",
    "### Supervised learning \n",
    "### Unsupervised learning \n",
    "<p style=\"font-family: Arial; font-size:1.75em;color:#2462C0; font-style:bold\"><br>\n",
    "Linear regression</p>\n",
    "<p style=\"font-family: Arial; font-size:1.75em;color:#2462C0; font-style:bold\"><br>\n",
    "Loading datasets</p>\n",
    "<p style=\"font-family: Arial; font-size:1.75em;color:#2462C0; font-style:bold\"><br>\n",
    "An example of reshaping data: the digits dataset</p>\n",
    "<p style=\"font-family: Arial; font-size:1.75em;color:#2462C0; font-style:bold\"><br>\n",
    "Learning and predicting</p>\n",
    "\n",
    "\n",
    "\n",
    "### In this section, we introduce the machine learning vocabulary that we use throughout scikit-learn and provide some simple learning examples.\n",
    "### In general, a learning problem considers a set of n samples of data and then tries to predict properties of unknown data. If each sample is more than a single number and, for instance, a multi-dimensional entry (aka multivariate data), it is said to have several attributes or features.\n",
    "### We can separate learning problems in a few large categories:\n",
    "\n",
    "<p style=\"font-family: Arial; font-size:1.75em;color:#2462C0; font-style:bold\"><br>\n",
    "Supervised learning, in which the data comes with additional attributes that we want to predict.</p>\n",
    "\n",
    "## This problem can be either:\n",
    "\n",
    "## Classification: samples belong to two or more classes and we want to learn from already labeled data how to predict the class of unlabeled data. \n",
    "### An example of classification problem would be the handwritten digit recognition example, in which the aim is to assign each input vector to one of a finite number of discrete categories. \n",
    "### Another way to think of classification is as a discrete (as opposed to continuous) form of supervised learning where one has a limited number of categories and for each of the n samples provided, one is to try to label them with the correct category or class.\n",
    "\n",
    "## Regression: if the desired output consists of one or more continuous variables, then the task is called regression. \n",
    "### An example of a regression problem would be the prediction of the length of a salmon as a function of its age and weight.\n",
    "\n",
    "<p style=\"font-family: Arial; font-size:1.75em;color:#2462C0; font-style:bold\"><br>\n",
    "Unsupervised learning, in which the training data consists of a set of input vectors x without any corresponding target values.</p>\n",
    " \n",
    "### The goal in such problems may be to discover groups of similar examples within the data, where it is called clustering, or to determine the distribution of data within the input space, known as density estimation, or to project the data from a high-dimensional space down to two or three dimensions for the purpose of visualization."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Training set and testing set\n",
    "### Machine learning is about learning some properties of a data set and applying them to new data. This is why a common practice in machine learning to evaluate an algorithm is to split the data at hand into two sets, one that we call the training set on which we learn data properties and one that we call the testing set on which we test these properties."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### The following are a set of methods intended for regression in which the target value is expected to be a linear combination of the input variables. \n",
    "\n",
    "### Across the module, we designate the vector $W$ as coef_ \n",
    "### and $w_0$ as intercept_."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<p style=\"font-family: Arial; font-size:1.75em;color:#2462C0; font-style:bold\"><br>\n",
    "All machine learning models in scikit-learn are implemented in their own class, which are called Estimator classes. The linear regression algorithm is implemented in the LinearRegression class in the linear_model module.</p>\n",
    "\n",
    "<p style=\"font-family: Arial; font-size:1.75em;color:#2462C0; font-style:bold\"><br>\n",
    "Before we can use the model, we need to instantiate the class into an object. This is\n",
    "when we will set any parameters of the model. No parameter is needed.</p>\n",
    "\n",
    "<p style=\"font-family: Arial; font-size:1.75em;color:#2462C0; font-style:bold\"><br>\n",
    "The reg object encapsulates the algorithm to build the model from the training data,\n",
    "as well the algorithm to make predictions on new data points.</p>\n",
    "\n",
    "<p style=\"font-family: Arial; font-size:1.75em;color:#2462C0; font-style:bold\"><br>\n",
    "It will also hold the information the algorithm has extracted from the training data.\n",
    "In the case of LinearRegression, it will store the coefficients (coef_) and the intercept (intercept_).</p>\n",
    "\n",
    "<p style=\"font-family: Arial; font-size:1.75em;color:#2462C0; font-style:bold\"><br>\n",
    "We evaluate the model using the score method, that computes the accuracy of the\n",
    "model. The fit, predict and score methods are the common interface to supervised models in scikit-learn.</p>\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "LinearRegression(copy_X=True, fit_intercept=True, n_jobs=1, normalize=False)"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn import linear_model\n",
    "reg = linear_model.LinearRegression()\n",
    "reg.fit ([[0, 0.1], [1.2, 0.8], [1.8, 2.2]], [0.1, 0.8, 1.8])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### LinearRegression will take in its fit method arrays X, y and will store the coefficients w of the linear model in its coef_ member:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 0.22222222,  0.61904762])"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "reg.coef_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.038095238095238071"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "reg.intercept_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using matplotlib backend: Qt5Agg\n"
     ]
    }
   ],
   "source": [
    "%matplotlib\n",
    "import numpy as np\n",
    "import matplotlib\n",
    "import matplotlib.pyplot as plt\n",
    "from pylab import *\n",
    "#This is the 3D plotting toolkit\n",
    "from mpl_toolkits.mplot3d import Axes3D\n",
    "\n",
    "fig = plt.figure()\n",
    "ax = plt.axes(projection='3d')\n",
    "\n",
    "\n",
    "x1 = [0,1.2,1.8]\n",
    "x2 = [0.1,0.8,2.2]\n",
    "y = [0.1, 0.8, 1.8]\n",
    "z= [0,2]\n",
    "\n",
    "ax.scatter(x1, x2, y, alpha=0.9, marker = 'o', s = 150, c='r')\n",
    "\n",
    "slope = reg.coef_\n",
    "intercept = reg.intercept_\n",
    "\n",
    "# Create a list of values in the best fit line\n",
    "abline_values = [slope[0] * x1[i] + slope[1] * x2[i] + intercept for i in z] \n",
    "\n",
    "\n",
    "plt.plot([x1[0], x1[2]], [x1[0], x2[2]],[abline_values[0], abline_values[1]],'b')\n",
    "\n",
    "\n",
    "\n",
    "ax.set_xlabel('x1')\n",
    "ax.set_ylabel('x2')\n",
    "ax.set_zlabel('y')\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Loading example datasets\n",
    "## scikit-learn comes with a few standard datasets, for instance the iris and digits datasets for classification and the boston house prices dataset for regression.\n",
    "## In the following, we load the iris and digits datasets. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn import datasets\n",
    "iris = datasets.load_iris()\n",
    "digits = datasets.load_digits()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\nThe :mod:`sklearn.datasets` module includes utilities to load datasets,\\nincluding methods to load and fetch popular reference datasets. It also\\nfeatures some artificial data generators.\\n'"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "datasets.__doc__"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## A dataset is a dictionary-like object that holds all the data and some metadata about the data. \n",
    "### This data is stored in the .data member, which is a n_samples, n_features array. In the case of supervised problem, one or more response variables are stored in the .target member. \n",
    "### In the case of the digits dataset, digits.data gives access to the features that can be used to classify the digits samples:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# An example of reshaping data: the digits dataset\n",
    "### The digits dataset consists of 1797 images, where each one is an 8x8 pixel image representing a hand-written digit\n",
    "### In order to utilize an 8x8 figure like this, we’d have to first transform it into a feature vector with length 64. \n",
    "### This can be seen as an example of feature extractor\n",
    "\n",
    "# Parameterized mapping from images to label scores\n",
    "# with a Linear Classifier\n",
    "\n",
    "### The first component of this approach is to define the score function that maps the pixel values of an image to confidence scores for each class.\n",
    "### We have a training set of N = 1797 images, each with D = 8 x 8 = 64 pixels, and K = 10, since there are 10 distinct classes (0, 1, 2, ..., 9 = 10 digits)\n",
    "### We will define the score function $$f:R^{D}↦R^{K}$$ that maps the raw image pixels to class scores."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "type(digits.data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "print(\"n_samples, n_features\")\n",
    "print(digits.data.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1797, 8, 8)"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "digits.images.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAPgAAAD8CAYAAABaQGkdAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nYgQOJEbgQGIEDiT2P9zCoP29PRDeAAAAAElFTkSuQmCC\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x1aa919a37f0>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import pylab as pl\n",
    "pl.imshow(digits.images[5], cmap=pl.cm.gray_r)\n",
    "pl.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAACwAAAAwCAYAAABqkJjhAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz\nAAALEgAACxIB0t1+/AAAAsZJREFUaIHtmLFrU1EUh79fLe1QJBlSHap9reCiS6FBF4e4SDcrOLhI\nt04O7eZY/A8KIlhEOolbi5OiQ3FwMUKCKAZqeaBdJENKJ0U5Ds2LLyHNu1ffQwLvg0fefTn3nV8u\n555zc2RmDBMj/1uAL7ngrMkFZ00uOGucBEtakNSQtCvpbtaiBmJmAy/gBPAZOAeMAXXgQtK8rK5R\nh990Cdg1sz0ASU+B68DH4yaUSiWbmZkBoNFoADA+Pg5A9LyXMAxpNptKEuMieAr4Eht/BS73Gkla\nBpYBpqenqVarAFQqlS6hm5ubfZ2Uy2UHKTiFxE3gUWx8G7g/aM78/LxFBEFgQRAY0HUFQWBx2nMS\n9bhsun3gbGx8pv3MiWKxSLFY7IwLhQKFQoFKpUKr1aLVarm+CnDLEm+B85JmJY0Bt4BnXl5SJDGG\nzeynpDvAC44yxmMz++DqIIrder0OwMHBAQBzc3NdK++Ka+F4AHwHDoEb3l5SxCVLRFw1s6avg+3t\nbQB2dnYAqNVqAKyurnZsVlZWnN83dKXZdYUNeCXpF/DQzDZ8HUX5OE4Yhr6vcRZ8xcz2JZ0CXkr6\nZGav4wa9hSMiColog62trXW+W1xc9BbsFBJmtt/+/AZscVSue202zKxsZuXJyUlvIa4krrCkCWDE\nzA7b99eAe64Oos22vr7e9XxpaalvmCThEhKngS1Jkf0TM3vu7SkllEVfQtIh0EgwKwHxNBmYWWIs\n+eRhHxpmNvD4JamaZNOPocvDueA2LoXFu/hARpsuS/KQcGkJSAolvZdUk1T1cpDmX3AcWwJACJT+\nxkfaK9xpCZjZDyBqCaRG2oL7tQSm+thFx9V37VOeM1lVuiQSj6vHkfYKO7UEXI6rx5LyphsF9oBZ\n/my6iz02E8DJ2P0bYMHVR6oh4dgS+Kfjal7psiYXnDW54KzJBWfN0An+DZ7RjowQJOBeAAAAAElF\nTkSuQmCC\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x1aa92200438>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "%matplotlib inline\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "#Display the first digit\n",
    "plt.figure(1, figsize=(0.25, 0.25))\n",
    "plt.imshow(digits.images[5], cmap=plt.cm.gray_r, interpolation='nearest')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([  0.,   0.,  12.,  10.,   0.,   0.,   0.,   0.,   0.,   0.,  14.,\n",
       "        16.,  16.,  14.,   0.,   0.,   0.,   0.,  13.,  16.,  15.,  10.,\n",
       "         1.,   0.,   0.,   0.,  11.,  16.,  16.,   7.,   0.,   0.,   0.,\n",
       "         0.,   0.,   4.,   7.,  16.,   7.,   0.,   0.,   0.,   0.,   0.,\n",
       "         4.,  16.,   9.,   0.,   0.,   0.,   5.,   4.,  12.,  16.,   4.,\n",
       "         0.,   0.,   0.,   9.,  16.,  16.,  10.,   0.,   0.])"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "digits.data[5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "print(digits.data)  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "digits.data[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "type(digits.data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### digits.target gives the ground truth for the digit dataset, that is the number corresponding to each digit image that we are trying to learn:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "digits.target"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "digits.target[0:19]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Shape of the data arrays\n",
    "The data is always a 2D array, shape (n_samples, n_features), although the original data may have had a different shape. In the case of the digits, each original sample is an image of shape (8, 8) and can be accessed using:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "digits.images[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "digits.images[1]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<p style=\"font-family: Arial; font-size:1.75em;color:#2462C0; font-style:bold\"><br>\n",
    "The Review of Important Concepts covered in Week 3 </p>\n",
    "\n",
    "# Learning and Predicting of Multiclass Classification\n",
    "\n",
    "### In the case of the digits dataset, the task is to predict, given an image, which digit it represents. We are given samples of each of the 10 possible classes (the digits zero through nine) on which we fit an estimator to be able to predict the classes to which unseen samples belong.\n",
    "<img src='pixelspace.jpeg' width='50%'>\n",
    "\n",
    "### The linear classifier has the functional form of  $ f(x_i, W, b) =  W x_i + b$\n",
    "\n",
    "### In the above equation, we are assuming that the image $x_i$ has all of its pixels flattened out to a single column vector of shape [D x 1]. The matrix W (of size [K x D]), and the vector b (of size [K x 1]) are the parameters of the function. \n",
    "### $x_i$ contains all pixels in the i-th image flattened into a single [64 x 1] column, W is [10 x 64] and b is [10 x 1], so 64 numbers come into the function (the raw pixel values) and 10 numbers come out (the class scores). \n",
    "### The parameters in W are often called the weights, and b is called the bias vector because it influences the output scores, but without interacting with the actual data $x_i$. \n",
    "\n",
    "<img src='imagemap.jpg' width='70%'/>\n",
    "\n",
    "### An example of mapping an image to class scores. For the sake of visualization, we assume the image only has 4 pixels (4 monochrome pixels, we are not considering color channels in this example here), and that we have 3 classes (red (cat), green (dog), blue (ship) class). \n",
    "<p style=\"font-family: Arial; font-size:1.75em;color:#2462C0; font-style:bold\"><br>\n",
    "How good is the weight vector $W$ in the example above?</p>\n",
    "\n",
    "### The linear classifier has the functional form of  $ f(x_i, W, b) =  W x_i + b$\n",
    "\n",
    "### Many linear classification models are for binary classification only, and don’t extend naturally to the multiclass case (with the exception of logistic regression). A common technique to extend a binary classification algorithm to a multiclass classification algorithm is the one-vs.-rest approach. \n",
    "### In the one-vs.-rest approach, a binary model is learned for each class that tries to separate that class from all of the other classes, resulting in as many binary models as there are classes. \n",
    "### To make a prediction, all binary classifiers are run on a test point. The classifier that has the highest score on its single class “wins,” and this class label is returned as the prediction.\n",
    "<img src='one-vs-rest classifiers.png' width='80%'/>\n",
    "\n",
    "## How are predictions made based on the three binary linear classifiers shown here?\n",
    "<br>\n",
    "## 1. Should we take the one-vs-rest approach based on the binary classifiers? Yes or No?\n",
    "<br><br><br>\n",
    "## 2. But what about the little triangle in the middle of the plot where all three binary classifiers classiy points there as \"rest\"?\n",
    "\n",
    "\n",
    "<br><br><br><br>\n",
    "\n",
    "## The binary classifier is not the decision boundry dividing two classes here! It provides a class score and there are 3 class scores provided by 3 binary classifiers.\n",
    "## Which class would a point in the middle assigned to?\n",
    "## The one with the highest score provided the classifier.\n",
    "<img src='decision boundaries from one-vs-rest.png' width='70%'/>\n",
    "\n",
    "\n",
    "### Unlike this example (with a SVM classifier) which treats the outputs $f(x_i,W,b)$ as (possibly difficult to interpret) scores for each class, the Softmax classifier gives a slightly more intuitive output (normalized class probabilities), which is also why Logistic Regression extends naturally from binary classification to multiclass classification.\n",
    "\n",
    "\n",
    "### In scikit-learn, an estimator for classification is a Python object that implements the methods fit(X, y) and predict(T).\n",
    "\n",
    "### An example of an estimator is the class sklearn.linear_model.LogisticRegression that implements Logistic Regression Classification. \n",
    "\n",
    "### The constructor of an estimator takes as arguments the parameters of the model, but for the time being, we will consider the estimator as a black box.\n",
    "\n",
    "### from sklearn.linear_model import LogisticRegression\n",
    "### model = LogisticRegression()\n",
    "### clf = model.fit(X_train,y_train)\n",
    "### Now, we have had our classifier/learner/model trained, we can discord our traing set, evaluate the model, and make prodictions.\n",
    "### clf.score(X_test, y_test)\n",
    "### clf.predict(X_new)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<p style=\"font-family: Arial; font-size:3.75em;color:purple; font-style:bold\"><br>\n",
    "Introduction to Support <br><br><br>Vector Machines <br><br><br>with scikit-learn<br>\n",
    "<br><br>Week 12<br></p>\n",
    "\n",
    "<p style=\"font-family: Arial; font-size:1.75em;color:#2462C0; font-style:bold\"><br>\n",
    "A support vector machine (SVM) is a supervised learning technique from the field of machine learning applicable to both classification and regression.</p>\n",
    "\n",
    "\n",
    "### In machine learning, support vector machines (SVMs) are supervised learning models with associated learning algorithms that analyze data used for classification and regression analysis. \n",
    "\n",
    "### Given a set of training examples, each marked as belonging to one or the other of two categories, an SVM training algorithm builds a model that assigns new examples to one category or the other, making it a non-probabilistic binary linear classifier. \n",
    "\n",
    "### An SVM model is a representation of the examples as points in space, mapped so that the examples of the separate categories are divided by a clear gap that is as wide as possible. New examples are then mapped into that same space and predicted to belong to a category based on which side of the gap they fall.\n",
    "\n",
    "### In addition to performing linear classification, SVMs can efficiently perform a non-linear classification using what is called the kernel trick, implicitly mapping their inputs into high-dimensional feature spaces."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Classification\n",
    "\n",
    "### Classifying data is a common task in machine learning. \n",
    "### Suppose some given data points each belong to one of two classes, and the goal is to decide which class a new data point will be in. \n",
    "### In the case of support vector machines, a data point is viewed as a p-dimensional vector (a list of p numbers), and we want to know whether we can separate such points with a (p-1)-dimensional hyperplane. \n",
    "### This is called a linear classifier. There are many hyperplanes that might classify the data. One reasonable choice as the best hyperplane is the one that represents the largest separation, or margin, between the two classes. So we choose the hyperplane so that the distance from it to the nearest data point on each side is maximized. If such a hyperplane exists, it is known as the maximum-margin hyperplane and the linear classifier it defines is known as a maximum margin classifier; or equivalently, the perceptron of optimal stability.\n",
    "\n",
    "### More formally, a support vector machine constructs a hyperplane or set of hyperplanes in a high- or infinite-dimensional space, which can be used for classification, regression, or other tasks. Intuitively, a good separation is achieved by the hyperplane that has the largest distance to the nearest training-data point of any class (so-called functional margin), since in general the larger the margin the lower the generalization error of the classifier."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn import svm\n",
    "clf = svm.SVC(gamma=0.001, C=100)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Choosing the parameters of the model\n",
    "### In this example we set the value of gamma manually. It is possible to automatically find good values for the parameters by using tools such as grid search and cross validation.\n",
    "### We call our estimator instance clf, as it is a classifier. \n",
    "### It now must be fitted to the model, that is, it must learn from the model. \n",
    "### This is done by passing our training set to the fit method. \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## The loss functions we’ll look at in this class are usually defined over very high-dimensional spaces (in this example, a linear classifier weight matrix is of size [10 x 64] for a total of 640 parameters), making them difficult to visualize.\n",
    "\n",
    "<p style=\"font-family: Arial; font-size:1.75em;color:#2462C0; font-style:bold\"><br>\n",
    "This is for advanced students only.</p>\n",
    "### Now, let's explain the piecewise-linear structure of the loss function by examining the math. For a single example we have:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$ L_i = \\sum_{j\\neq y_i} \\left[ \\max(0, w_j^Tx_i - w_{y_i}^Tx_i + 1) \\right] $\n",
    "\n",
    "### It is clear from the equation that the data loss for each example is a sum of (zero-thresholded due to the $max(0,−)$ function) linear functions of $W$. Moreover, each row of $W$ (i.e. $w_j$) sometimes has a positive sign in front of it (when it corresponds to a wrong class for an example), and sometimes a negative sign (when it corresponds to the correct class for that example). To make this more explicit, consider a simple dataset that contains three 1-dimensional points and three classes. The full SVM loss (without regularization) becomes:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "\\begin{align}\n",
    "L_0 = & \\max(0, w_1^Tx_0 - w_0^Tx_0 + 1) + \\max(0, w_2^Tx_0 - w_0^Tx_0 + 1) \\\\\\\\\n",
    "L_1 = & \\max(0, w_0^Tx_1 - w_1^Tx_1 + 1) + \\max(0, w_2^Tx_1 - w_1^Tx_1 + 1) \\\\\\\\\n",
    "L_2 = & \\max(0, w_0^Tx_2 - w_2^Tx_2 + 1) + \\max(0, w_1^Tx_2 - w_2^Tx_2 + 1) \\\\\\\\\n",
    "L = & (L_0 + L_1 + L_2)/3\n",
    "\\end{align}\n",
    "### Since these examples are 1-dimensional, the data $x_i$ and weights $w_j$ are numbers. Looking at, for instance, $w_0$, some terms above are linear functions of  $w_0$ and each is clamped at zero. We can visualize this as follows:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src='svmbowl.png' width='70%'>\n",
    "\n",
    "### 1-dimensional illustration of the data loss. The x-axis is a single weight and the y-axis is the loss. The data loss is a sum of multiple terms, each of which is either independent of a particular weight, or a linear function of it that is thresholded at zero. The full SVM data loss is a 640-dimensional version of this shape.\n",
    "\n",
    "### The linear classifier weight matrix is of size [10 x 64] for a total of 640 parameters\n",
    "\n",
    "<p style=\"font-family: Arial; font-size:1.75em;color:#2462C0; font-style:bold\"><br>\n",
    "The end of advanced section.</p>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### As a training set, let us use all the images of our dataset apart from the last one. \n",
    "### We select this training set with the [:-1] Python syntax, which produces a new array that contains all but the last entry of digits.data:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "clf.fit(digits.data[:-1], digits.target[:-1])  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Now you can predict new values, in particular, we can ask to the classifier what is the digit of our last image in the digits dataset, which we have not used to train the classifier:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "clf.predict(digits.data[-1:])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#The corresponding image is the following:\n",
    "pl.imshow(digits.images[-1], cmap=pl.cm.gray_r)\n",
    "pl.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[alt text](image_001.png \"Title\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## As you can see, it is a challenging task: the images are of poor resolution. Do you agree with the classifier?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "#Display the first digit\n",
    "plt.figure(1, figsize=(0.2, 0.2))\n",
    "plt.imshow(digits.images[-1], cmap=plt.cm.gray_r, interpolation='nearest')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model Evaluation: Quantifying the Quality of Predictions\n",
    "## Classification Report\n",
    "### Compute precision, recall, F-measure and support for each class\n",
    "<img src='tpfp.png' width='70%'/>\n",
    "#### The precision is the ratio tp / (tp + fp) where tp is the number of true positives and fp the number of false positives. The precision is intuitively the ability of the classifier not to label as positive a sample that is negative.\n",
    "#### The recall is the ratio tp / (tp + fn) where tp is the number of true positives and fn the number of false negatives. The recall is intuitively the ability of the classifier to find all the positive samples.\n",
    "#### The F-beta score can be interpreted as a weighted harmonic mean of the precision and recall, where an F-beta score reaches its best value at 1 and worst score at 0.\n",
    "#### The F-beta score weights recall more than precision by a factor of beta, the strength of recall versus precision in the F-score. beta == 1.0 means recall and precision are equally important.\n",
    "#### The support is the number of occurrences of each class in y_true (Ground truth (correct) target values)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Confusion Matrix\n",
    "#### A confusion matrix is a table that is often used to describe the performance of a classification model (or \"classifier\") on a set of test data for which the true values are known. \n",
    "\n",
    "<img src='confusion_matrix.png' width='50%'\\>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Classification report for classifier SVC(C=1.0, cache_size=200, class_weight=None, coef0=0.0,\n",
      "  decision_function_shape=None, degree=3, gamma=0.001, kernel='rbf',\n",
      "  max_iter=-1, probability=False, random_state=None, shrinking=True,\n",
      "  tol=0.001, verbose=False):\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0       1.00      0.99      0.99        79\n",
      "          1       0.99      0.96      0.97        80\n",
      "          2       0.99      0.99      0.99        77\n",
      "          3       0.97      0.86      0.91        79\n",
      "          4       0.99      0.95      0.97        83\n",
      "          5       0.95      0.99      0.97        82\n",
      "          6       0.99      0.99      0.99        80\n",
      "          7       0.95      1.00      0.98        80\n",
      "          8       0.94      1.00      0.97        76\n",
      "          9       0.94      0.98      0.96        81\n",
      "\n",
      "avg / total       0.97      0.97      0.97       797\n",
      "\n",
      "\n",
      "Confusion matrix:\n",
      "[[78  0  0  0  1  0  0  0  0  0]\n",
      " [ 0 77  1  0  0  0  0  0  1  1]\n",
      " [ 0  0 76  1  0  0  0  0  0  0]\n",
      " [ 0  0  0 68  0  3  0  4  4  0]\n",
      " [ 0  0  0  0 79  0  0  0  0  4]\n",
      " [ 0  0  0  0  0 81  1  0  0  0]\n",
      " [ 0  1  0  0  0  0 79  0  0  0]\n",
      " [ 0  0  0  0  0  0  0 80  0  0]\n",
      " [ 0  0  0  0  0  0  0  0 76  0]\n",
      " [ 0  0  0  1  0  1  0  0  0 79]]\n"
     ]
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAW4AAADuCAYAAAAZZe3jAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x1746dcd5358>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Standard scientific Python imports\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Import datasets, classifiers and performance metrics\n",
    "from sklearn import datasets, svm, metrics\n",
    "\n",
    "# The digits dataset\n",
    "digits = datasets.load_digits()\n",
    "\n",
    "# The data that we are interested in is made of 8x8 images of digits, let's\n",
    "# have a look at the first 3 images, stored in the `images` attribute of the\n",
    "# dataset.  If we were working from image files, we could load them using\n",
    "# pylab.imread.  Note that each image must have the same size. For these\n",
    "# images, we know which digit they represent: it is given in the 'target' of\n",
    "# the dataset.\n",
    "images_and_labels = list(zip(digits.images, digits.target))\n",
    "for index, (image, label) in enumerate(images_and_labels[:4]):\n",
    "    plt.subplot(2, 4, index + 1)\n",
    "    plt.axis('off')\n",
    "    plt.imshow(image, cmap=plt.cm.gray_r, interpolation='nearest')\n",
    "    plt.title('Training: %i' % label)\n",
    "\n",
    "# To apply a classifier on this data, we need to flatten the image, to\n",
    "# turn the data in a (samples, feature) matrix:\n",
    "n_samples = len(digits.images)\n",
    "data = digits.images.reshape((n_samples, -1))\n",
    "\n",
    "# Create a classifier: a support vector classifier\n",
    "classifier = svm.SVC(gamma=0.001)\n",
    "\n",
    "# We learn the digits on the first 1000 digits\n",
    "classifier.fit(data[:1000], digits.target[:1000])\n",
    "\n",
    "# Now predict the value of the digit on 1001~1797:\n",
    "expected = digits.target[1000:]\n",
    "predicted = classifier.predict(data[1000:])\n",
    "\n",
    "print(\"Classification report for classifier %s:\\n%s\\n\"\n",
    "      % (classifier, metrics.classification_report(expected, predicted)))\n",
    "print(\"Confusion matrix:\\n%s\" % metrics.confusion_matrix(expected, predicted))\n",
    "\n",
    "images_and_predictions = list(zip(digits.images[1000:], predicted))\n",
    "for index, (image, prediction) in enumerate(images_and_predictions[:4]):\n",
    "    plt.subplot(2, 4, index + 5)\n",
    "    plt.axis('off')\n",
    "    plt.imshow(image, cmap=plt.cm.gray_r, interpolation='nearest')\n",
    "    plt.title('Prediction: %i' % prediction)\n",
    "\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
