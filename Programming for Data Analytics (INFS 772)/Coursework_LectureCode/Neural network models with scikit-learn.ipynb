{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "<p style=\"font-family: Arial; font-size:2.5em;color:purple; font-style:bold\"><br>\n",
    "Neural Network Models with scikit-learn<br>\n",
    "<br><br>INFS 772 Spring 2019 Week 13<br></p>\n",
    "\n",
    "<p style=\"font-family: Arial; font-size:1.5em;color:#2462C0; font-style:bold\"><br>\n",
    "A High-level Overview of Learning: From Loss Function to Stochastic Gradient Descent (SGD)</p>\n",
    "\n",
    "\n",
    "<br><br><br><b>The loss function takes the predictions of the network and the true target (what you wanted the network to output), and computes a distance score, capturing how well the network has done on this specific example.\n",
    "\n",
    "<br><br><br>The loss function, averaged over all the training examples, can be seen as a kind of hilly landscape in the high-dimensional space of weight values. The negative gradient vector indicates the direction of steepest descent in this landscape, taking it closer to a minimum, where the output error is low on average.\n",
    "<br>\n",
    "<br><br><br>The fundamental trick in machine (deep) learning is to use this score as a feedback signal to adjust the value of the weights by a little bit, in a direction that would lower the loss score for the current example. This adjustment is the job of the \"optimizer\", which implements what is called the \"backpropagation\" algorithm, the central algorithm in neural networks (deep) learning.</b><br><br><br>\n",
    "\n",
    "<img src='neuron_model.jpeg' width='50%'>\n",
    "\n",
    "<center><b>Mathematical model of a neuron</b></center><br>\n",
    "Forget anything you may have read so far about hypothetical links\n",
    "between deep learning and biology. For our purposes, neural networks (deep) learning is merely a\n",
    "mathematical framework for learning representations from data.\n",
    "<br><br><br>\n",
    "\n",
    "<img src='perceptron_img1.png' width='35%'>\n",
    "\n",
    "<center><b>Perceptron Classifier</b></center>\n",
    "\n",
    "<br><br><img src='learningw.png' width='40%'><br>\n",
    "<img src='perceptron_img2.png' width='80%'>\n",
    "\n",
    "<center><b>Geometric Intuition of Perceptron Classifier</b></center><br><br>\n",
    "<img src='howtooptimize.png' width='50%'><br>\n",
    "A  general  approach  is  to  use iterative optimization,  which  essentially  starts  at  some  starting  point w (say, all zeros), and tries to tweak w so that the objective function value decreases.\n",
    "<br>To  do  this,  we  will  rely  on  the  gradient  of  the  function,  which  tells  us  which  direction  to  move  in  to decrease  the  objective  the  most.  The  gradient  is  a  valuable  piece  of  information,  especially  since  we  will often be optimizing in high dimensions (d on the order of thousands).\n",
    "<br>This iterative optimization procedure is called gradient descent.  Gradient descent has two hyperparameters, the step size η (which specifies how aggressively we want to pursue a direction) and the number of iterations T.  Let’s not worry about how to set them, but you can think of T = 100 and η = 0.1 for now.\n",
    "<br><br>\n",
    "<img src='gradientdescent.png' width='50%'><br>\n",
    "The answer is <b>stochastic gradient descent (SGD)</b>. Rather than looping through all the training examples to  compute  a  single  gradient  and  making  one  step,  SGD  loops  through  the  examples (x, y) and  updates the  weights w based  on each example.   Each  update  is  not  as  good  because  we’re  only  looking  at  one example rather than all the examples, but we can make many more updates this way.\n",
    "<br>In  practice,  we  often  find  that  just  performing  one  pass  over  the  training  examples  with  SGD,  touching each example once, often performs comparably to taking ten passes over the data with GD.\n",
    "<br>There are other variants of SGD. You can randomize the order in which you loop over the training data in each iteration, which is useful.\n",
    "<br><br>\n",
    "<img src='sgd.png' width='50%'><br>\n",
    "\n",
    "<p style=\"font-family: Arial; font-size:1.5em;color:#2462C0; font-style:bold\"><br>\n",
    "Multi-layer Perceptron</p>\n",
    "\n",
    "\n",
    "#### In the context of neural networks, a perceptron is an artificial neuron using the unit step function as the activation function. The perceptron algorithm is also termed the single-layer perceptron, to distinguish it from a multilayer perceptron, which is a misnomer for a more complicated neural network. As a linear classifier, the single-layer perceptron is the simplest feedforward neural network.\n",
    "\n",
    "#### Multi-layer Perceptron (MLP) is a supervised learning algorithm that learns a function $f(.): R^m -> R^o$ by training on a dataset, where $m$ is the number of dimensions for input and $o$ is the number of dimensions for output. \n",
    "#### Given a set of features $X = {x_1, x_2, ..., x_m}$ and a target $y$, it can learn a non-linear function approximator for either classification or regression. \n",
    "#### It is different from logistic regression, in that between the input and the output layer, there can be one or more non-linear layers, called hidden layers. The Figure below shows a one hidden layer MLP with scalar output."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"mlp.png\" width=\"30%\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### The module contains the public attributes coefs_ and intercepts_. coefs_ is a list of weight matrices, where weight matrix at index $i$ represents the weights between layer $i$ and layer $i+1$. intercepts_ is a list of bias vectors, where the vector at index $i$ represents the bias values added to layer $i+1$.\n",
    "\n",
    "### The advantages of Multi-layer Perceptron are:\n",
    "#### Capability to learn non-linear models.\n",
    "\n",
    "### The disadvantages of Multi-layer Perceptron (MLP) include:\n",
    "#### MLP with hidden layers have a non-convex loss function where there exists more than one local minimum. Therefore different random weight initializations can lead to different validation accuracy.\n",
    "#### MLP requires tuning a number of hyperparameters such as the number of hidden neurons, layers, and iterations.\n",
    "#### MLP is sensitive to feature scaling."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Classification\n",
    "\n",
    "### Class MLPClassifier implements a multi-layer perceptron (MLP) algorithm that trains using Backpropagation.\n",
    "### This model optimizes the log-loss function using LBFGS or stochastic gradient descent.\n",
    "### MLP trains on two arrays: array X of size (n_samples, n_features), which holds the training samples represented as feature vectors; and array y of size (n_samples,), which holds the target values (class labels) for the training samples:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### The default solver ‘adam’ works pretty well on relatively large datasets in terms of both training time and validation score. \n",
    "#### The method computes individual adaptive learning rates for different parameters from estimates of first and second moments of the gradients.\n",
    "### For small datasets, however, ‘lbfgs’ can converge faster and perform better.\n",
    "<img src='relu.png' width='40%'>\n",
    "<center><b>The Rectified Linear Unit (ReLU) Activation function used by hidden layers in MLP</b></center>\n",
    "\n",
    "#### Alpha is a parameter for regularization term, aka penalty term, that combats overfitting by constraining the size of the weights. Increasing alpha may fix high variance (a sign of overfitting) by encouraging smaller weights\n",
    "http://scikit-learn.org/stable/modules/generated/sklearn.neural_network.MLPClassifier.html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "MLPClassifier(activation='relu', alpha=1e-05, batch_size='auto', beta_1=0.9,\n",
       "       beta_2=0.999, early_stopping=False, epsilon=1e-08,\n",
       "       hidden_layer_sizes=(5, 2), learning_rate='constant',\n",
       "       learning_rate_init=0.001, max_iter=200, momentum=0.9,\n",
       "       nesterovs_momentum=True, power_t=0.5, random_state=1, shuffle=True,\n",
       "       solver='lbfgs', tol=0.0001, validation_fraction=0.1, verbose=False,\n",
       "       warm_start=False)"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.neural_network import MLPClassifier\n",
    "X = [[0., 0.], [1., 1.],[1., 0.], [0., 1.]]\n",
    "y = [0, 0, 1, 1]\n",
    "clf = MLPClassifier(solver='lbfgs', alpha=1e-5,\n",
    "    hidden_layer_sizes=(5, 2), random_state=1)\n",
    "\n",
    "clf.fit(X, y)  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0, 1])"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# After fitting (training), the model can predict labels for new samples:\n",
    "\n",
    "clf.predict([[2., 2.], [2., 0.]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(2, 5), (5, 2), (2, 1)]"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# MLP can fit a non-linear model to the training data. clf.coefs_ contains \n",
    "# the weight matrices that constitute the model parameters:\n",
    "\n",
    "[coef.shape for coef in clf.coefs_]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[array([[-0.15333045,  0.91922774, -0.92371103, -4.81107371, -0.65274027],\n",
       "        [-0.75329499, -0.99731064, -0.2853798 , -5.48064596,  0.0717273 ]]),\n",
       " array([[ 0.31499749, -0.15280789],\n",
       "        [-3.5688568 , -0.66450911],\n",
       "        [-0.55786158,  0.55572927],\n",
       "        [ 5.40687496, -0.34476315],\n",
       "        [ 0.35538234,  0.69550872]]),\n",
       " array([[-3.59162073],\n",
       "        [-0.93194639]])]"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "clf.coefs_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[array([-0.14962269,  0.07491987, -0.5472481 ,  4.84883135, -0.87510813]),\n",
       " array([ 2.5568664 , -0.76834882]),\n",
       " array([9.18319357])]"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "clf.intercepts_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'logistic'"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Name of the output activation function\n",
    "clf.out_activation_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Currently, MLPClassifier supports only the Cross-Entropy loss function and allows probability estimates by running the predict_proba method.\n",
    "### MLP trains using Backpropagation. More precisely, it trains using some form of gradient descent and the gradients are calculated using Backpropagation. For classification, it minimizes the Cross-Entropy loss function, giving a vector of probability estimates P(y|x) per sample x:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0.5000252, 0.4999748],\n",
       "       [0.5000252, 0.4999748]])"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "clf.predict_proba([[2., 2.], [1., 2.]])  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### MLPClassifier supports multi-class classification by applying Softmax as the output function.\n",
    "### Further, the model supports multi-label classification in which a sample can belong to more than one class. For each class, the raw output passes through the logistic function. Values larger or equal to 0.5 are rounded to 1, otherwise to 0. For a predicted output of a sample, the indices where the value is 1 represents the assigned classes of that sample:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "MLPClassifier(activation='relu', alpha=1e-05, batch_size='auto', beta_1=0.9,\n",
       "       beta_2=0.999, early_stopping=False, epsilon=1e-08,\n",
       "       hidden_layer_sizes=(15,), learning_rate='constant',\n",
       "       learning_rate_init=0.001, max_iter=200, momentum=0.9,\n",
       "       nesterovs_momentum=True, power_t=0.5, random_state=1, shuffle=True,\n",
       "       solver='lbfgs', tol=0.0001, validation_fraction=0.1, verbose=False,\n",
       "       warm_start=False)"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X = [[0., 0.], [1., 1.]]\n",
    "y = [[0, 1], [1, 1]]\n",
    "clf = MLPClassifier(solver='lbfgs', alpha=1e-5,\n",
    "    hidden_layer_sizes=(15,), random_state=1)\n",
    "\n",
    "clf.fit(X, y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[1, 1]])"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "clf.predict([[1., 2.]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0, 1]])"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "clf.predict([[0., 0.]])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### MLPClassifier supports multi-class classification by applying Softmax as the output function. Let's try to use it to classify the iris dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\users\\dzeng\\appdata\\local\\programs\\python\\python36\\lib\\site-packages\\sklearn\\neural_network\\multilayer_perceptron.py:564: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (500) reached and the optimization hasn't converged yet.\n",
      "  % self.max_iter, ConvergenceWarning)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "MLPClassifier(activation='relu', alpha=0.0001, batch_size='auto', beta_1=0.9,\n",
       "       beta_2=0.999, early_stopping=False, epsilon=1e-08,\n",
       "       hidden_layer_sizes=(100,), learning_rate='constant',\n",
       "       learning_rate_init=0.001, max_iter=500, momentum=0.9,\n",
       "       nesterovs_momentum=True, power_t=0.5, random_state=None,\n",
       "       shuffle=True, solver='adam', tol=0.0001, validation_fraction=0.1,\n",
       "       verbose=False, warm_start=False)"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.neural_network import MLPClassifier\n",
    "from sklearn.datasets import load_iris\n",
    "import numpy as np\n",
    "\n",
    "X,Y = load_iris().data, load_iris().target\n",
    "\n",
    "mlp = MLPClassifier(max_iter=500)\n",
    "mlp.fit(X, Y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(4, 100), (100, 3)]"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "[coef.shape for coef in mlp.coefs_]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2]\n",
      "[[4.84116398e-15 1.15324758e-10 1.00000000e+00]]\n",
      "sum:  1.0\n"
     ]
    }
   ],
   "source": [
    "new_data = np.array([3.1,  2.5,  8.4,  2.2])\n",
    "predicted = mlp.predict(new_data.reshape(1, -1))\n",
    "print(predicted)\n",
    "predicted_proba = mlp.predict_proba(new_data.reshape(1, -1))\n",
    "print(predicted_proba) \n",
    "print(\"sum: \", np.sum(predicted_proba)) # What is the sum here?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'softmax'"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mlp.out_activation_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<p style=\"font-family: Arial; font-size:1.5em;color:#2462C0; font-style:bold\">\n",
    "Visualization of MLP weights on MNIST</p><br><br>\n",
    "\n",
    "#### The MNIST database of handwritten digits has a training set of 60,000 examples, and a test set of 10,000 examples. \n",
    "#### The digits have been size-normalized and centered in a fixed-size image.\n",
    "\n",
    "Sometimes looking at the learned coefficients of a neural network can provide insight into the learning behavior. For example if weights look unstructured, maybe some were not used at all, or if very large coefficients exist, maybe regularization was too low or the learning rate too high.\n",
    "\n",
    "This example shows how to plot some of the first layer weights in a MLPClassifier trained on the MNIST dataset.\n",
    "\n",
    "The input data consists of 28x28 pixel handwritten digits, leading to 784 features in the dataset. Therefore the first layer weight matrix have the shape (784, hidden_layer_sizes[0]). We can therefore visualize a single column of the weight matrix as a 28x28 pixel image.\n",
    "\n",
    "To make the example run faster, we use very few hidden units, and train only for a very short time. Training longer would result in weights with a much smoother spatial appearance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 1, loss = 0.32212731\n",
      "Iteration 2, loss = 0.15738787\n",
      "Iteration 3, loss = 0.11647274\n",
      "Iteration 4, loss = 0.09631113\n",
      "Iteration 5, loss = 0.08074513\n",
      "Iteration 6, loss = 0.07163224\n",
      "Iteration 7, loss = 0.06351392\n",
      "Iteration 8, loss = 0.05694146\n",
      "Iteration 9, loss = 0.05213487\n",
      "Iteration 10, loss = 0.04708320\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\users\\dzeng\\appdata\\local\\programs\\python\\python36\\lib\\site-packages\\sklearn\\neural_network\\multilayer_perceptron.py:564: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (10) reached and the optimization hasn't converged yet.\n",
      "  % self.max_iter, ConvergenceWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training set score: 0.985733\n",
      "Test set score: 0.971000\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<matplotlib.figure.Figure at 0x19d71d0e3c8>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "from sklearn.datasets import fetch_mldata\n",
    "from sklearn.neural_network import MLPClassifier\n",
    "\n",
    "mnist = fetch_mldata(\"MNIST original\")\n",
    "# rescale the data, use the traditional train/test split\n",
    "X, y = mnist.data / 255., mnist.target\n",
    "X_train, X_test = X[:60000], X[60000:]\n",
    "y_train, y_test = y[:60000], y[60000:]\n",
    "\n",
    "# mlp = MLPClassifier(hidden_layer_sizes=(100, 100), max_iter=400, alpha=1e-4,\n",
    "#                     solver='sgd', verbose=10, tol=1e-4, random_state=1)\n",
    "mlp = MLPClassifier(hidden_layer_sizes=(50,), max_iter=10, alpha=1e-4,\n",
    "                    solver='sgd', verbose=10, tol=1e-4, random_state=1,\n",
    "                    learning_rate_init=.1)\n",
    "\n",
    "mlp.fit(X_train, y_train)\n",
    "print(\"Training set score: %f\" % mlp.score(X_train, y_train))\n",
    "print(\"Test set score: %f\" % mlp.score(X_test, y_test))\n",
    "\n",
    "fig, axes = plt.subplots(4, 4)\n",
    "# use global min / max to ensure all weights are shown on the same scale\n",
    "vmin, vmax = mlp.coefs_[0].min(), mlp.coefs_[0].max()\n",
    "for coef, ax in zip(mlp.coefs_[0].T, axes.ravel()):\n",
    "    ax.matshow(coef.reshape(28, 28), cmap=plt.cm.gray, vmin=.5 * vmin,\n",
    "               vmax=.5 * vmax)\n",
    "    ax.set_xticks(())\n",
    "    ax.set_yticks(())\n",
    "\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
