{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Andrew Marshall\n",
    "INFS 770\n",
    "04/19/2019\n",
    "Assignment 4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Built-in/Generic Imports\n",
    "import os\n",
    "import sys\n",
    "#\n",
    "\n",
    "# Libraries\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import re\n",
    "import nltk\n",
    "nltk.download('stopwords')\n",
    "nltk.download('punkt')\n",
    "nltk.download('wordnet')\n",
    "import gensim\n",
    "\n",
    "#import matplotlib.pyplot as plt\n",
    "#import mglearn.plots\n",
    "#\n",
    "\n",
    "# Modules\n",
    "from nltk import word_tokenize \n",
    "from sklearn.cluster import KMeans\n",
    "from gensim.models import LdaModel, LsiModel\n",
    "from sklearn.linear_model import SGDClassifier\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn import metrics\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.decomposition import PCA\n",
    "\n",
    "#from nltk.stem import WordNetLemmatizer \n",
    "#from sklearn import tree\n",
    "\n",
    "#from sklearn.datasets import load_breast_cancer\n",
    "#from sklearn.tree import DecisionTreeClassifier\n",
    "#from sklearn.datasets import make_blobs\n",
    "\n",
    "#from sklearn.svm import SVC\n",
    "#from sklearn.metrics import classification_report,confusion_matrix\n",
    "#from sklearn.metrics import precision_score\n",
    "#\n",
    "\n",
    "#Suppress scientific notation\n",
    "np.set_printoptions(suppress=True)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Q1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#import file for dataset\n",
    "#reviews = open(\"amazon_reviews_texts.csv\",\"r\") \n",
    "reviews = pd.read_csv('amazon_review_texts.csv')\n",
    "reviews.head()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "reviews['score'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "reviews['category'].value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Q2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get a set of stopwords\n",
    "stopwords = set(nltk.corpus.stopwords.words(\"english\"))\n",
    "\n",
    "def before_token(documents):\n",
    "    # conver words to lower case\n",
    "    lower = map(str.lower, documents)\n",
    "    # remove punctuations\n",
    "    punctuationless = list(map(lambda x: \" \".join(re.findall('\\\\b\\\\w\\\\w+\\\\b',x)), lower))\n",
    "    # remove numbers\n",
    "    return list(map(lambda x:re.sub('\\\\b[0-9]+\\\\b', '', x), punctuationless))\n",
    "\n",
    "# initialize a stemmer\n",
    "stemmer = nltk.stem.PorterStemmer()\n",
    "\n",
    "# initialize a container of token frequencies\n",
    "fdist = nltk.FreqDist()\n",
    "\n",
    "\n",
    "# define a function that preprocess a single document and returns a list of tokens\n",
    "def preprocess(doc):\n",
    "    tokens = []\n",
    "    for token in doc.split():\n",
    "        if token not in stopwords:\n",
    "            tokens.append(stemmer.stem(token))\n",
    "    return tokens\n",
    "            \n",
    "# preprocess all documents\n",
    "processed = list(map(preprocess, before_token(reviews['text'])))\n",
    "print(processed[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# calculate the token frequency\n",
    "# the FreqDist function takes in a list of tokens and return a dict containg unique tokens and frequency\n",
    "fdist = nltk.FreqDist([token for doc in processed for token in doc])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Unique tokens: %d\" % fdist.B())\n",
    "print(\"Total tokens: %d\" % fdist.N())\n",
    "print(\"Tokens occurred only once: %d\" % len(fdist.hapaxes()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Top 10 words\n",
    "fdist.tabulate(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Based on the results above, the word \"would\" would not be as useful given that it has the lowest value."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Q3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "processed_doc = list(map(\" \".join, processed))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# vectorize\n",
    "vectorizer = TfidfVectorizer(max_df=0.8,norm=\"l2\",stop_words=stopwords)\n",
    "#vectorizer = TfidfVectorizer(stop_words=\"english\")\n",
    "corpus_vect = vectorizer.fit_transform(processed_doc)\n",
    "print(corpus_vect) # sparse matrix\n",
    "df_vect = pd.DataFrame(corpus_vect.toarray(), columns=vectorizer.get_feature_names())\n",
    "print(df_vect)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# examine the mapping of words to feature indexes\n",
    "vectorizer.vocabulary_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Q4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"n_samples: %d, n_features: %d\" % corpus_vect.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Number of categories\n",
    "categories = len(reviews['category'].drop_duplicates())\n",
    "\n",
    "km = KMeans(n_clusters=categories, max_iter=100, random_state=42)\n",
    "# km = KMeans(n_clusters=8, max_iter=100, random_state=54321)\n",
    "km.fit(corpus_vect)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "km.transform(corpus_vect)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "km.predict(corpus_vect[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# examine the representative words for each cluster\n",
    "order_centroids = km.cluster_centers_.argsort()[:, ::-1]\n",
    "terms = vectorizer.get_feature_names()\n",
    "for i in range(categories):\n",
    "    print(\"Cluster %d:\" % i)\n",
    "    for ind in order_centroids[i, :10]:\n",
    "        print(' %s' % terms[ind])\n",
    "    print"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Three of the clusters appear to be accurate, but Cluster 0 does not accurately describe the category \"automotive\". I believe the word \"bed\", which refers to a truck bed most likely led to the inclusion of \"mattress\" and \"sleep\", which means context is a factor."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Q5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# convert the vectorized data to a gensim corpus object\n",
    "corpus = gensim.matutils.Sparse2Corpus(corpus_vect, documents_columns=False)\n",
    "id2word = dict((v,k) for k,v in vectorizer.vocabulary_.items())\n",
    "print(id2word)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# build the lda model\n",
    "lda = LdaModel(corpus, num_topics=4,id2word=id2word, passes=10)\n",
    "print(lda.print_topics())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The words appear to describe their respective topics based on the assigned weights, though not as well as the clustering method did. \n",
    "Also, the LDA is prone to random results depending on which random seed is used. It also would require a larger sample size \n",
    "to generate more accurate results."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Q6"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 5-fold cross validation\n",
    "skf = StratifiedKFold(n_splits=5)\n",
    "fold = 0\n",
    "for train_index, test_index in skf.split(np.array(processed_doc), reviews.category):\n",
    "    fold += 1\n",
    "    print(\"Fold %d\" % fold)\n",
    "    # partition\n",
    "    train_x, test_x = np.array(processed_doc)[train_index], np.array(processed_doc)[test_index]\n",
    "    train_y, test_y = reviews.category[train_index], reviews.category[test_index]\n",
    "    # vectorize\n",
    "    vectorizer = TfidfVectorizer(max_df=0.8,min_df=2,stop_words=stopwords)\n",
    "    X = vectorizer.fit_transform(train_x)\n",
    "    print(\"Number of features: %d\" % len(vectorizer.vocabulary_))\n",
    "    X_test = vectorizer.transform(test_x)\n",
    "     # train model\n",
    "    clf = SGDClassifier(random_state=fold,max_iter = 1000, tol = 1e-3)\n",
    "    clf.fit(X, train_y)\n",
    "    # predict\n",
    "    pred = clf.predict(X_test)\n",
    "    # classification results\n",
    "    for line in metrics.classification_report(test_y, pred).split(\"\\n\"):\n",
    "        print(line)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Q7"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "scores= reviews['score']\n",
    "sat = [4,5]\n",
    "notsat = [1,2,3]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "satisfaction = []\n",
    "i = 0\n",
    "while(i < len(scores)):\n",
    "    if scores[i] in sat :\n",
    "        satisfaction.append(1)\n",
    "    elif scores[i] in notsat:\n",
    "        satisfaction.append(0)\n",
    "    i += 1\n",
    "\n",
    "\n",
    "#satisfaction = np.array(satisfaction)   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sat_df = pd.DataFrame({'satisfaction':satisfaction})\n",
    "sat_df\n",
    "reviews[\"satisfaction\"] = sat_df\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 5-fold cross validation\n",
    "skf = StratifiedKFold(n_splits=5)\n",
    "fold = 0\n",
    "for train_index, test_index in skf.split(np.array(processed_doc), reviews.satisfaction):\n",
    "    fold += 1\n",
    "    print(\"Fold %d\" % fold)\n",
    "    # partition\n",
    "    train_x, test_x = np.array(processed_doc)[train_index], np.array(processed_doc)[test_index]\n",
    "    train_y, test_y = reviews.satisfaction[train_index], reviews.satisfaction[test_index]\n",
    "    # vectorize\n",
    "    vectorizer = TfidfVectorizer(max_df=0.8,min_df=2,stop_words=stopwords)\n",
    "    X = vectorizer.fit_transform(train_x)\n",
    "    print(\"Number of features: %d\" % len(vectorizer.vocabulary_))\n",
    "    X_test = vectorizer.transform(test_x)\n",
    "     # train model\n",
    "    clf = SGDClassifier(random_state=fold,max_iter = 1000, tol = 1e-3)\n",
    "    clf.fit(X, train_y)\n",
    "    # predict\n",
    "    pred = clf.predict(X_test)\n",
    "    # classification results\n",
    "    for line in metrics.classification_report(test_y, pred).split(\"\\n\"):\n",
    "        print(line)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Q8"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# read the lexicon\n",
    "lexicon = dict()\n",
    "\n",
    "# read postive words\n",
    "with open(\"opinion-lexicon-English\\\\negative-words.txt\", \"r\") as in_file:\n",
    "    for line in in_file.readlines():\n",
    "        if not line.startswith(\";\") and line != \"\\n\":\n",
    "            lexicon[line.strip()] = -1\n",
    "\n",
    "# read negative words\n",
    "with open(\"opinion-lexicon-English\\\\positive-words.txt\", \"r\") as in_file:\n",
    "    for line in in_file.readlines():\n",
    "        if not line.startswith(\";\") and line != \"\\n\":\n",
    "            lexicon[line.strip()] = 1\n",
    "\n",
    "# print the top 5 entries\n",
    "for i, (k, v) in enumerate(lexicon.items()):\n",
    "    print(k, v)\n",
    "    if i > 4: break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vocabulary=lexicon.keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 5-fold cross validation\n",
    "\n",
    "skf = StratifiedKFold(n_splits=5)\n",
    "fold = 0\n",
    "for train_index, test_index in skf.split(np.array(processed_doc),reviews.satisfaction):\n",
    "    fold += 1\n",
    "    print(\"Fold %d\" % fold)\n",
    "    # partition\n",
    "    train_x, test_x = np.array(processed_doc)[train_index], np.array(processed_doc)[test_index]\n",
    "    train_y, test_y = reviews.satisfaction[train_index], reviews.satisfaction[test_index]\n",
    "    # vectorize\n",
    "    vectorizer = TfidfVectorizer(max_df=0.8,min_df=2,stop_words=stopwords,vocabulary=vocabulary)\n",
    "    X = vectorizer.fit_transform(train_x)\n",
    "    print(\"Number of features: %d\" % len(vectorizer.vocabulary_))\n",
    "    X_test = vectorizer.transform(test_x)\n",
    "     # train model\n",
    "    clf = SGDClassifier(random_state=fold,max_iter = 1000, tol = 1e-3)\n",
    "    clf.fit(X, train_y)\n",
    "    # predict\n",
    "    pred = clf.predict(X_test)\n",
    "    # classification results\n",
    "    for line in metrics.classification_report(test_y, pred).split(\"\\n\"):\n",
    "        print(line)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The average F1-score has in fact decreased. Context can sometimes factor into how words are interpreted during the analysis."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Q9"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#X_std = StandardScaler().fit_transform(X) # you need to do standardization, since pca is sensitive to the relative scaling of the original variables\n",
    "vectorizer = TfidfVectorizer(max_df=0.8,min_df=2,stop_words=stopwords)\n",
    "X = vectorizer.fit_transform(processed_doc).todense()\n",
    "y = reviews.satisfaction\n",
    "X = StandardScaler().fit_transform(X)\n",
    "print(len(X[0]))\n",
    "pca = PCA(svd_solver='randomized',whiten=True).fit(X)\n",
    "print(pca.explained_variance_ratio_)\n",
    "sumofvariance=0.0\n",
    "n_components = 0\n",
    "for item in pca.explained_variance_ratio_:\n",
    "    sumofvariance += item\n",
    "    n_components+=1\n",
    "    if sumofvariance>=0.9:\n",
    "        break\n",
    "print(n_components)\n",
    "X_train_pca = pca.transform(X)\n",
    "train_x, test_x, train_y, test_y = train_test_split(X_train_pca,y,test_size=0.2, stratify=y, random_state=42)\n",
    "pca = PCA(n_components=n_components, svd_solver='randomized',whiten=True).fit(train_x)\n",
    "clf = SGDClassifier(random_state=fold,max_iter = 1000, tol = 1e-3)\n",
    "clf.fit(train_x, train_y)\n",
    "pred_y = clf.predict(test_x)\n",
    "for line in metrics.classification_report(test_y, pred_y).split(\"\\n\"):\n",
    "    print(line)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "PCA is known as Principcal Component Analysis; \n",
    "It reduces the number of features provided with data to what are known as Principal Components and then uses those Principal Components to make\n",
    "its predictions. It is recommended that its independent variables be \n",
    "standardized because this method has a weakness again relative scaling of the original variables."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 5-fold cross validation\n",
    "X = pca.components_.transpose()\n",
    "y = reviews.satisfaction\n",
    "skf = StratifiedKFold(n_splits=5)\n",
    "fold = 0\n",
    "for train_index, test_index in skf.split(X,y):\n",
    "    fold += 1\n",
    "    print(\"Fold %d\" % fold)\n",
    "    # partition\n",
    "    train_x, test_x = X[train_index], X[test_index]\n",
    "    train_y, test_y = y[train_index], y[test_index]\n",
    "    print(\"Number of features: %d\" % n_components)\n",
    "    # train model\n",
    "    clf = SGDClassifier(random_state=fold,max_iter = 1000, tol = 1e-3)\n",
    "    clf.fit(train_x,train_y)\n",
    "    # predict\n",
    "    pred = clf.predict(test_x)\n",
    "    # classification results\n",
    "    for line in metrics.classification_report(test_y, pred).split(\"\\n\"):\n",
    "        print(line)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The average F1-score has in fact decreased after the cross-validation. \n",
    "A possible reason for this is that the latent concepts of components can not always be interpreted accurately."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
